[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics and Probability for Engineering",
    "section": "",
    "text": "Preface\nThis is an eBook developed for the undergrad students of engineering faculty to provide knowledge of statistics, probability, probability distributions, statistical inference, correlation and regression analysis,stochastic process and design and analysis of experiment with applications in the engineering field.\nThe whole book is developed using R Programming Language (R Core Team 2024).\n\n\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "1Introduction_to_statistics_1.html",
    "href": "1Introduction_to_statistics_1.html",
    "title": "1¬† Introduction to statistics and Data analysis",
    "section": "",
    "text": "1.1 Descriptive statistics\nAn important aspect of dealing with data is organizing and summarizing the data in ways that facilitate its interpretation and subsequent analysis. This aspect of statistics is called descriptive statistics. Usually, data are summarized both numerically and graphically. ¬†In this module, our focus will be concentrated on the following topics:\n(i) Numerical Summaries of Data (ii) Graphical summaries (Histogram and ¬†Box Plot)\nNumerical summaries of the data\nOften to summarize data we use some numerical measures like measures of central tendency, measures of location and measures of variation. The common numerical summaries are:\nWhile studying these numerical summaries, we consider sample data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to statistics and Data analysis</span>"
    ]
  },
  {
    "objectID": "1Introduction_to_statistics_1.html#descriptive-statistics",
    "href": "1Introduction_to_statistics_1.html#descriptive-statistics",
    "title": "1¬† Introduction to statistics and Data analysis",
    "section": "",
    "text": "1) Measures of central tendency\n2) Measures of relative standing: Quantile\n3) Measures of variability\n\n\n\n\nMean/ Arithmetic mean/average, Median, Mode etc\na)Percentiles, Quartiles etc.\na) Range, Inter-quartile range (IQR), Variance and standard deviation, Coefficient of variation (CV)% etc.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to statistics and Data analysis</span>"
    ]
  },
  {
    "objectID": "1Introduction_to_statistics_1.html#measures-of-central-tendency-and-quantile",
    "href": "1Introduction_to_statistics_1.html#measures-of-central-tendency-and-quantile",
    "title": "1¬† Introduction to statistics and Data analysis",
    "section": "1.2 Measures of central tendency and quantile",
    "text": "1.2 Measures of central tendency and quantile\n\n1.2.1 Mean\n\nSample mean: Suppose \\(n\\) observation of a variable \\(X\\) is drawn from a population. Then the sample mean is denoted by \\(\\bar x\\) and\n\\[\n\\bar x =\\frac{\\sum x}{n}\n\\]\nThe sample mean \\(\\bar x\\) is a sample statistic.\nPopulation mean: Suppose in a population there are \\(N\\) values of variable \\(X\\). Then the population mean is denoted by \\(\\mu\\) and\n\\[\n\\mu =\\frac{\\sum x}{N}\n\\]\nThe \\(\\bar x\\) is a point estimator of the population mean \\(\\mu\\).\n\nExample 1.1: Eight prototype units are produced and their pull-off forces measured, resulting in the following data (in pounds): 12 .6, 12. 9, 13. 4, 12. 3, 13. 6, 13 .5, 12. 6, 13. 1.\nSuppose,\\(X=\\)pull-off forces (in pounds). So,\n\\(X=\\{x_1,x_2,...,x_8\\}=\\{12.6,12.9,‚Ä¶,13.1\\}\\)\nThe sample mean is, pounds\n\\[\n\\bar x=\\frac{\\sum x}{n}=\\frac{12.6+12.9+...+13.1}{8}=\\frac{104}{8}=13.0 \\ \\ pounds\n\\]\n\n\n1.2.2 Median\nThe median is another measure of central location. The median is the value in the middle when the data are arranged in ascending order (smallest value to largest value).\n\nFor an odd number of observations, median is the middle value\nFor an even number of observations, median is the average of the two middle values\n\nExample 1.2: CPU time of 9 jobs (in seconds):\nData: 59, 139, 46, 37, 42, 30, 55, 56, 82\nArranged/sorted data: 30, 37, 42, 46, 55, 56, 59, 82, 139\nMedian=55; whereas the Mean=60.67\nExample 1.3 : CPU time of 10 jobs (in seconds): 59, 139, 46, 37, 42, 30, 55, 56, 36, 82\nArranged/sorted data: 30, 36, 37, 42, 46, 55, 56, 59, 82, 139.\nMedian = (46+55)/2=50.5; whereas Mean=58.2.\n\n\n\n\n\n\nNote\n\n\n\nNotice that the median is unaffected by the size of the largest CPU time. It impiles that, mean is affected by extreme value or outlier but median is not.\n\n\n\n\n1.2.3 Mode\nis the most frequently occurring data value.\n\n\n1.2.4 Percentiles\nPercentiles divide the whole data set into approximately 100 equal parts. So, there are 99 percentiles -\\(P_1, P_2,...,P_{99}\\). In this lecture \\(j^{th}\\) percentile will be denoted by \\(P_j\\) .\nWe can compute the \\(j^{th}\\) percentile as follows:\n\\[\nP_j=(j*\\frac{n+1}{100})^{th} \\ \\ value; \\ \\ j=1,2,...,99.\n\\]\nExample 1.4: Compute the 25th and 60th percentile from the following data:\nCPU time of 9 jobs (in seconds): 59, 46, 37, 42, 30, 55, 56, 36, and 82.\nSolution: Here, \\(n=9\\).\nSorted data: ¬†30, 36, 37, 42, 46, 55, 56, 59, and 82.\nSo, \\(P_{25}=(25*\\frac{9+1}{100})^{th} \\ \\ value=2.5^{th} \\ \\ value\\)\n\\(=2^{nd} \\ \\ value +0.5(3^{rd}-2^{nd})=36+0.5(37-36)=36.5\\).\nInterpretation: \\(P_{25}=36.5\\) implies that approximately 25% of the total observations lie below or equal to 36.5.\nSimilarly, \\(P_{60}=(60*\\frac{9+1}{100})^{th}\\ \\ value=6^{th}\\ \\ value=55\\).\nInterpretation: \\(P_{60}=55\\) implies that approximately 60% of the total observations lie below or equal to 55.\n\n\n1.2.5 Quartiles\nIt is often desirable to divide data into four parts, with each part containing approximately one-fourth, or 25% of the observations. The division points are referred to as the quartiles and are defined as\nQ1 = first quartile, or 25th percentile\nQ2 = second quartile, or 50th percentile (also the median)\nQ3 = third quartile, or 75th percentile.\nExample 1.5: Here is the monthly starting salary ($) of 12 graduates:\n3450, 3550, 3650, 3480, 3355, 3310, 3490, 3730, 3540, 3925, 3520, 3480\nCompute Q1 and Q3 of the above data (Will be solved in class).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to statistics and Data analysis</span>"
    ]
  },
  {
    "objectID": "1Introduction_to_statistics_1.html#measures-of-variability",
    "href": "1Introduction_to_statistics_1.html#measures-of-variability",
    "title": "1¬† Introduction to statistics and Data analysis",
    "section": "1.3 Measures of variability",
    "text": "1.3 Measures of variability\nVariability in data means lack of uniformity. It is also referred to as spread, scatter, or dispersion. We turn now to a discussion of some commonly used measures of variability.\n\n1.3.1 Range\n\\[\nRange=Largest \\ \\ value-Smallest \\ \\ value\n\\]\n\nHeavily influenced by extreme values\n\n\n\n1.3.2 Inter-quartile-range (IQR)\n\\[\nIQR=Q_3-Q_1\n\\]\n\nIt exhibits the variability in the middle 50% of the observations.\nThe interquartile range is less sensitive to the extreme values in the sample than is the ordinary sample range.\n\nDetection of outliers using 1.5(IQR) rule\nIn this method, we discuss fences.\n\nThe lower fence of distribution is ùíçùíêùíòùíÜùíì ùíáùíÜùíèùíÑùíÜ, \\(lf=Q_1-1.5(IQR)\\)\nThe upper fence of distribution is ùíñùíëùíëùíÜùíì ùíáùíÜùíèùíÑùíÜ, \\(uf=Q_3+1.5(IQR)\\)\nIf any value lies outside the interval \\([lf,uf]\\); then it will be considered an outlier.\n\nExample 1.6: The following data set represents the number of new computer accounts registered during ten consecutive days.¬† 43, 37, 50, 51, 58, 105, 52, 45, 45, 10. Check for outliers using the 1.5(IQR) rule.\n\n\n1.3.3 Variance and standard deviation\nThe variability or scatter in the data around mean may be described by the variance and standard deviation.\n\nPopulation variance: If \\(x_1, x_2, ...,x_N\\) is a population of \\(N\\) observations , the population variance is\n\\[\n\\sigma^2=\\frac{(x_1-\\mu)^2+(x_2-\\mu)^2+...+(x_N-\\mu)^2}{N}\n\\]\n\\[\n=\\frac{\\sum_{i=1}^N (x_i-\\mu)^2} {N}=\\frac{\\sum_{i=1}^N x_i^2}{N}-\\mu^2\n\\]\nSample variance: If \\(x_1, x_2, ...,x_n\\) is a sample of \\(n\\) observations , the sample variance is\n\n\\[\ns^2=\\frac{(x_1-\\bar x)^2+(x_2-\\bar x)^2+...+(x_n-\\bar x)^2}{n-1}=\\frac{\\sum_{i=1}^n (x_i-\\bar x)^2} {n-1}\n\\]\n\nAn alternative formula for the computation of the sample variance is:\n\n\\[\ns^2=\\frac{\\sum x_i^2-n\\bar x^2}{n-1}\n\\] where, \\(\\sum x_i^2=x_1^2+x_2^2+...+x_n^2\\)\nExample 1.7: Eight prototype units are produced and their pull-off forces measured, resulting in the following data (in pounds): 12 .6, 12. 9, 13. 4, 12. 3, 13. 6, 13 .5, 12. 6, 13. Compute sample variance.\nSolution:\n\n\n\nTable¬†1.1: Computation of the sample variance for the pull-off force data\n\n\n\n\n\n\n\n\n\n\n\nPull-off force(\\(x_i\\))\nSample mean, \\(\\bar x\\)\n\\((x_i-\\bar x)\\)\n\\((x_i-\\bar x)^2\\)\n\n\n\n\n12.6\n13\n-0.4\n0.16\n\n\n12.9\n13\n-0.1\n0.01\n\n\n13.4\n13\n0.4\n0.16\n\n\n12.3\n13\n-0.7\n0.49\n\n\n13.6\n13\n0.6\n0.36\n\n\n13.5\n13\n0.5\n0.25\n\n\n12.6\n13\n-0.4\n0.16\n\n\n13.1\n13\n0.1\n0.01\n\n\n\n\n\\(\\sum (x_i-\\bar x ) =0\\)\n\\(\\sum (x_i-\\bar x )^2 =1.6\\)\n\n\n\n\n\n\n\\[\ns^2=\\frac{\\sum (x_i-\\bar x)^2}{n-1}=\\frac{1.6}{8-1}=0.2286 \\ \\ (pounds)^2\n\\]\nAlternative: Here \\(\\sum x^2 =12.6^2+12.9^2+\\cdot \\cdot\\cdot+13.1^2=1353.6\\)\n\\(\\bar x=13\\)\nSo, \\(s^2=\\frac{\\sum x^2-n\\times \\bar x^2}{n-1}=\\frac{1353.6-8\\times (13^2)}{8-1}=0.2285714\\approx 0.2286 \\ \\ (pounds)^2\\)\nStandard deviation\nThe standard deviation is defined to be the positive square root of the variance\n\nSample standard deviation=\\(s=\\sqrt {s^2}\\)\nPopulation standard deviation=\\(\\sigma =\\sqrt {\\sigma ^2}\\)\n\n\nThe sample standard deviation \\(s\\) is the estimator of population standard deviation \\(\\sigma\\).\n\nExample 1.8: The standard deviation of the previous example is :\n\\[ s=\\sqrt {0.2286} \\approx0.48 \\ \\ pounds\\]\n\n\n\n\n\n\nNote:\n\n\n\nThe standard deviation is easier to interpret than the variance because the standard deviation is measured in the same units as the data.\n\n\nFor example, the sample variance for the pull-off force data of prototype units is \\(s^2=0.2286 \\ \\ (pounds)^2\\).\nBecause the standard deviation is the square root of the variance, the units of the variance, pounds squared, are converted to pound in the standard deviation.\nThus, the standard deviation of the pull-off force data is 0.48 pounds. In other words, the standard deviation is measured in the same units as the original data. For this reason the standard deviation is more easily compared to the mean and other statistics that are measured in the same units as the original data.\n\n\n1.3.4 Coefficient of variation\nIn some situations we may be interested in a descriptive statistic that indicates how large the standard deviation is relative to the mean. This measure is called the coefficient of variation and is usually expressed as a percentage.\nCoefficient variation,\n\\[CV=\\frac{Standard \\ \\ deviation}{Mean}\\]\n\nThe coefficient of variation is a relative measure of variability; it measures the standard deviation relative to the mean.\nIn general, the coefficient of variation is a useful statistic for comparing the variability of variables that have different standard deviations and different means.\n\nExample 1.9: The table at the left shows the population heights (in inches) and weights (in pounds) of the members of a basketball team. Find the coefficient of variation for the heights and the weights. Then compare the results.\n\nDataCoefficient of variation\n\n\n\n\n\nHeights (inches)\nWeights (pounds)\n\n\n\n\n72\n180\n\n\n74\n168\n\n\n68\n225\n\n\n76\n201\n\n\n74\n189\n\n\n69\n192\n\n\n72\n197\n\n\n79\n162\n\n\n70\n174\n\n\n69\n171\n\n\n77\n185\n\n\n73\n210\n\n\n\n\n\nThe mean height \\(\\mu \\approx 72.8 \\ \\ inches\\) with a standard deviation \\(\\sigma =3.3 \\ \\ inches\\).\nThe coefficient of variation for the heights is\n\n\\(CV_{height}=\\frac{\\sigma}{\\mu}.100\\%=\\frac{3.3}{72.8} . 100\\% \\approx 4.5\\%\\).\nThe mean weight \\(\\mu \\approx 187.8 \\ \\ pounds\\) with a standard deviation \\(\\sigma =17.7 \\ \\ pounds\\).\nThe coefficient of variation for the weights is\n\\(CV_{weight}=\\frac{\\sigma}{\\mu}.100\\%=\\frac{17.7}{187.8}.100\\% \\approx9.4\\%\\)\nInterpretation The weights (9.4%) are more variable than the heights (4.5%).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to statistics and Data analysis</span>"
    ]
  },
  {
    "objectID": "1Introduction_to_statistics_1.html#graphical-summaryvisualization",
    "href": "1Introduction_to_statistics_1.html#graphical-summaryvisualization",
    "title": "1¬† Introduction to statistics and Data analysis",
    "section": "1.4 Graphical summary/visualization",
    "text": "1.4 Graphical summary/visualization\nBefore diving into advanced analysis we should have a look at the data. Because ‚Äù A picture is worth a thousand words‚Äù. Often a summary of a collection of data via a graphical display can provide insight regarding the system from which the data were taken.\n\n1.4.1 Frequency Distributions and Histograms\nTo construct a frequency distribution, we must divide the range of the data into intervals, which are usually called class intervals, cells, or bins, and count how many observations fall into each bin.\nThe histogram is a visual display of the frequency distribution.\n\nA frequency histogram consists of columns, one for each bin, whose height is determined by the number of observations in the bin. [Frequency distribution]\nA relative frequency histogram has the same shape but a different vertical scale. Its column heights represent the proportion of all data that appeared in each bin. [Relative frequency distribution]\n\nWhat is the appropriate size of bins?\nThere are several rules, but one of them is, the number of bins, suppose \\(k=\\sqrt n\\). We will discuss how to construct a frequency distribution, relative frequency distribution, and cumulative frequency distribution in the following example.\nExample 1.10: The following data are the joint temperatures of the O-rings (¬∞F) for each test firing or actual launch of the space shuttle rocket motor (from Presidential Commission on the Space Shuttle Challenger Accident, Vol. 1, pp.¬†129‚Äì131):\n\n67, 40, 58, 76, 58, 70, 72, 67, 75, 70, 57, 83, 53, 45, 70, 81, 78, 76, 67, 73, 61, 52, 31, 67, 79, 75, 69, 84, 68, 80\n\nTo construct frequency distribution and others follow the steps:\nStep-1: Find the number bins, \\(k=\\sqrt n=\\sqrt 30=5.48\\approx 6\\) (round to nearest integer).\nStep-2: Find the range of the data, \\(R=Maximum-Minimum=84-31=53\\)\nStep-3: Determine bin width, \\(w=\\frac{R}{k}=\\frac{53}{6}=8.83 \\approx 10\\)\nStep-4: Define the bins in exclusive method, starting from a suitable data value close to lowest value, for example [30, 40), [40, 50), and so on until we have the highest data value.\nStep-5: Now count the observation fall in each bin, using tally.\n\n\n\nTable¬†1.2: Frequency, Relative frequency, Cumulative frequency Distribution for the O-rings temperature data (n=30)\n\n\n\n\n\n\n\n\n\n\n\n\nTemperatures\n(in o F)\nTally\nFrequency (f)\nRelative frequency (rf)\nCumulative frequency (cf)\n\n\n\n\n[30,40)\n|\n1\n0.03\n1\n\n\n[40,50)\n||\n2\n0.07\n3\n\n\n[50,60)\n||||\n5\n0.17\n8\n\n\n[60,70)\n|||| ||\n7\n0.23\n15\n\n\n[70,80)\n|||| |||| |\n11\n0.37\n26\n\n\n[80,90)\n||||\n4\n0.13\n30\n\n\nTotal\n\nn=30\n1.00\n\n\n\n\n\n\n\n\n\n1.4.2 Histogram\nThe histogram provides a visual impression of the shape of the distribution of the measurements and information about the central tendency and scatter or dispersion in the data.\nThe histogram from the previous example is shown in Figure¬†1.1:\n\n\n\n\n\n\n\n\nFigure¬†1.1: Frequency histogram of temperature (¬∞F)\n\n\n\n\n\nHistogram and shape of the distribution\nWhen the sample size is large, the histogram can provide a reasonably reliable indicator of the general shape of the distribution or population of measurements from which the sample was drawn. (for detail see Montgomery and Runger (2014) ).\n\n\n\n\n\n\nFigure¬†1.2: Histograms for symmetric and skewed distributions\n\n\n\n\n\n1.4.3 Box-plot\nThe box plot is a graphical display that simultaneously describes several important features of a data set, such as center, spread, departure from symmetry, and identification of unusual observations or outliers.\nA box plot, sometimes called box-and-whisker plots, displays the three quartiles, the minimum/lower fence, and the maximum/upper fence of the data on a rectangular box, aligned either horizontally or vertically.\nIf any value falls outside the fences then it will shown as a circle in the box-plot.\nExample 1.11\n\nVariable-IVariable-II\n\n\nSuppose, X={10,15,14,18,17,12,16,15,19,21,32,12,58}\n\n\n\n\n\n\n\n\n\n\n\nSuppose, Y={5,8,14,15,18,17,16,14,15,21,21,35,17,16,20}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.4 Boxplot and skewness of the data\nWhen we discuss the frequency histogram we also learned about shape of the distribution. By visual inspection of boxplot we can also tell about the distribution shape of a variable. The following boxplots are the typycal examples of skewness of the data.\n\n\n\n\n\n\n\n\n\nComparative/Parallel box-plots\nBox plots are very useful in graphical comparisons among data sets because they have high visual impact and are easy to understand.\n\nFor instance, here the Life expectancy at birth (in year) of Afghanistan, Bangladesh, India and Pakistan is compared using a comparative box-plot from 1971 to 2007.\n\n\n\n\n\n\n\n\n\n\nQuestion: What is/are your observation(s) from this above comparative box-plot?",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to statistics and Data analysis</span>"
    ]
  },
  {
    "objectID": "1Introduction_to_statistics_1.html#exercises-practice-as-more-as-you-can",
    "href": "1Introduction_to_statistics_1.html#exercises-practice-as-more-as-you-can",
    "title": "1¬† Introduction to statistics and Data analysis",
    "section": "1.5 Exercises (Practice as more as you can)",
    "text": "1.5 Exercises (Practice as more as you can)\n1.1 Define statistics, population, sample, descriptive and inferential statistics.\n1.2 Define data. Discuss the types of data with example.\n1.3 What are the common measures of location? When median is preferable to mean?\n1.4 What are the common measures of variation?\n1.5 Define five-number summary. How to detect outliers using quartiles?\n1.6 The data contains the overall gallons per kilometer (GpK) of a medium-sized mobile home unit.\n35, 30, 37, 35, 34, 35, 35, 42,40, 37, 42, 38, 35, 34, 35, 34, 34.\nCalculate and explain Median. Find the value above which 15% GpK values lie.\n1.7 In automobile mileage and gasoline-consumption testing, 13 automobiles were road tested for 300 miles in city driving conditions. The following data were recorded for miles-per-gallon (mpg) performance.\n\n13.2, 14.4, 15.2, 15.3, 15.3, 15.3, 15.9, 16.0, 16.1, 16.2, 16.2, 16.7, 16.8\n\n\nConstruct a simple boxplot (in horizontal direction) of mpg. Are most of the automobiles‚Äô mpg relatively low?\nSuppose you want to buy a new car and you don‚Äôt afford enough money, so the car‚Äôs mileage must be in bottom 10%. What should be the mileage of your car based on this data?\nSuppose you want to buy a new car which mileage must be in top 10%. What should be the mileage of your car based on this data?\n\n1.8 The data set lists the prices (in dollars) of 20 portable global positioning system (GPS) navigators.\n\n128, 100, 180, 150, 200, 90, 340, 105, 85, 270, 200, 65, 230, 150, 150, 120, 130, 80, 230, 200\n\n\nConstruct a frequency distribution and percent frequency distribution of prices.\nDraw a frequency histogram. What is your observation about price data?\n\n1.9 The lengths of power failures, in minutes, are recorded in the following table.\n\n18, 135, 15, 90, 78, 69, 98, 102, 83, 55, 28\n\n\nCompute sample mean and standard deviation\nCompute sample median and IQR\nIf you want to be in the bottom 10% of power failures in your residence, then what should be the cutoff value of power failure lengths in minutes?\n\n1.10 Sample annual salaries (in thousands of dollars) for entry-level electrical engineers in Boston and Chicago are listed.\n\nBoston 70.4, 84.2, 58.5, 64.5, 71.6, 79.9, 88.3, 80.1, 69.9\nChicago 69.4, 71.5, 65.4, 59.9, 70.9, 68.5, 62.9, 70.1, 60.9\n\nFind the coefficient of variation for each of the two data sets. Then compare the results.\n1.11 The shear strengths of 20 spot welds in a titanium alloy are as follows:\n\n5408 5431 5475 5442 5376 5388 5459 5422 5416 5435\n5420 5429 5401 5446 5487 5416 5382 5357 5388 5457\n\nConstruct a frequency histogram of shear strength data. Conclude whether the shear strength is approximately symmetric or not.\n1.12 The ‚Äúcold start ignition time‚Äù of an automobile engine is being investigated by a gasoline manufacturer. The following times (in seconds) were obtained for a test vehicle:\n\n1.75, 1.92, 2.62, 2.35, 3.09, 3.15, 2.53, 1.91.\n\nCheck for outliers using the 1.5*IQR rule of cold start ignition time.\n1.13 The cylindrical compressive strength (in MPa) was measured for 11 beams. The results were:\n\n38.43, 38.43, 38.39, 38.83, 38.45, 38.35, 38.43, 38.31, 38.32, 38.48, 38.50.\n\ni) Construct a simple boxplot and comment about the shape of the distribution of compressive strength.\nii) Compute sample mean, standard deviation and coefficient of variation¬† (CV) compressive strength.\n1.14 The shear strengths (in N/m2) of 10 spot welds in a titanium alloy are:\n5408, 5431, 5475, 5442, 5376, 5388, 5459, 5422, 5416, 5435.\nCompute sample mean, standard deviation and coefficient of variation of shear strength.\n1.15 An article describes an experiment to test the yield strength of circular tubes with caps welded to the ends. The first 18 yields (in kN) are:\n\n96, 96, 102, 102, 102, 104, 104, 108, 126,126, 128, 128, 140, 156, 160, 160, 164,170.\n\nConstruct a simple box-plot of yield strength. Find the value above which top 10% yield strength lie.\n1.16 The cylindrical compressive strength (in MPa) was measured for 11 beams. The results were:\n\n38.43, 38.43, 38.39, 38.83, 38.45, 38.35, 38.43, 38.31, 38.32, 38.48, 38.50.\n\n\nCompute 1st quartile, 3rd quartile and IQR.\nCheck for outlier (s).\nConstruct a box-plot, show the outliers and comment about the symmetry of the distribution.\n\n1.17 The following sample data presents the thickness (√Ö) of a metal layer on 20 silicon wafers resulting from a chemical vapor deposition (CVD) process. Scientists believe that the thickness is usually normally distributed having a bell-shaped distribution for a smooth process. Construct a frequency histogram of metal thickness data. Conclude whether the process is smooth or not.\n\n468, 459, 450, 453, 473, 454, 458, 438, 447, 463,\n445, 466, 456, 434, 471, 437, 459, 445, 454, 423\n\n\n\n\n\nMontgomery, Douglas C., and George C. Runger. 2014. Applied Statistics and Probability for Engineers. Sixth edition. Hoboken, NJ: John Wiley; Sons, Inc.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to statistics and Data analysis</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html",
    "href": "2Probability_2.html",
    "title": "2¬† Probability",
    "section": "",
    "text": "2.1 Introduction\nA probability is the chance, or likelihood, that a particular event will occur. These are examples of events representing typical probability-type questions:\nTo answer these kind of questions in the face of uncertainty we need to study probability. To answer these type of questions which are raised in real life; at first we have to learn some basic concepts of probability.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html#introduction",
    "href": "2Probability_2.html#introduction",
    "title": "2¬† Probability",
    "section": "",
    "text": "How many customers will arrive in a super shop in next 30 minutes?\nWhat is probability that a stock price will rise or fall?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html#random-experiment",
    "href": "2Probability_2.html#random-experiment",
    "title": "2¬† Probability",
    "section": "2.2 Random experiment",
    "text": "2.2 Random experiment\nA random experiment is a process leading to two or more possible outcomes, without knowing exactly which outcome will occur.\nExample: Tossing a coin, throwing a dice, change in the stock prices etc.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html#sample-space",
    "href": "2Probability_2.html#sample-space",
    "title": "2¬† Probability",
    "section": "2.3 Sample space",
    "text": "2.3 Sample space\nA sample space is the collection of all outcomes of a random experiment. The sample space is usually denoted by \\(S\\) or Greek letter \\(\\Omega\\) (omega).\nExample 2.1:\n\nIf we toss a coin then the sample space is: \\(S=\\{H,T\\}\\)\nIf we toss 2 coins then the sample space is: \\(S=\\{HH,HT,TH,TT\\}\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html#event",
    "href": "2Probability_2.html#event",
    "title": "2¬† Probability",
    "section": "2.4 Event",
    "text": "2.4 Event\nAn event is a subset of a sample space.\nFor example suppose, \\(S=\\{HH,HT,TH,TT\\}\\) and \\(A=\\{ one \\ \\ head \\ \\ occurs  \\}\\). So, \\(A=\\{ HT,TH\\}\\).\nSince \\(A\\) is a subset of sample space \\(S\\), so \\(A\\) is an event.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html#complement-of-an-event",
    "href": "2Probability_2.html#complement-of-an-event",
    "title": "2¬† Probability",
    "section": "2.5 Complement of an event",
    "text": "2.5 Complement of an event\nThe complement of an event A with respect to Œ© is the subset of all elements of \\(\\Omega\\) that are not in A. We denote the complement of A by the symbol \\(A^C\\).\nExample 2.2: Consider the sample space:\n\\(\\Omega =\\{ 1,2,3,4,5,6\\}\\)\nLet, \\(A=\\{1,3,5 \\}\\). Then the complement of \\(A\\) is \\(A^C=\\Omega-A=\\{2,4,6\\}\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html#mutually-exclusive-events",
    "href": "2Probability_2.html#mutually-exclusive-events",
    "title": "2¬† Probability",
    "section": "2.6 Mutually exclusive events",
    "text": "2.6 Mutually exclusive events\nThe occurrence of one event means that none of the other events can occur at the same time.\nExample\n\nThe variable ‚ÄúEmployment status‚Äù presents mutually exclusive outcomes, employed and unemployed. An employee selected at random is either male or female but cannot be both.\nA manufactured part is acceptable or unacceptable. The part cannot be both acceptable and unacceptable at the same time.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html#axiomatic-definition-of-probability",
    "href": "2Probability_2.html#axiomatic-definition-of-probability",
    "title": "2¬† Probability",
    "section": "2.7 Axiomatic definition of Probability",
    "text": "2.7 Axiomatic definition of Probability\nThe probability of an event \\(A\\) is the sum of the weights of all sample points in \\(A\\). Therefore\n(a) \\(0 \\le P(A)\\le 1\\) ; \\(P(\\phi)=0\\) and \\(P(\\Omega)=1\\).\n(b) If \\(A_1, A_2,A_3,...\\) is a sequence of mutually exclusive events, then\n\\[\nP(A_1\\cup A_2 \\cup  A_3\\cup ...).=P(A_1)+P(A_2)+P(A_3)+...\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html#probability-of-an-event-classical-approach",
    "href": "2Probability_2.html#probability-of-an-event-classical-approach",
    "title": "2¬† Probability",
    "section": "2.8 Probability of an event (Classical approach)",
    "text": "2.8 Probability of an event (Classical approach)\nSuppose an event \\(A\\) is defined in the sample space \\(\\Omega\\). Then the probability of event \\(A\\) is defined as :\n\\[\nP(A)=\\frac{n(A)}{n(\\Omega)};\n\\]\nHere,\n\\(n(A)=\\) number of outcomes favorable to event \\(A\\);\n\\(n(\\Omega)=\\) total number of outcomes in the sample space \\(\\Omega\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html#probability-of-an-event-empirical-approach",
    "href": "2Probability_2.html#probability-of-an-event-empirical-approach",
    "title": "2¬† Probability",
    "section": "2.9 Probability of an event (Empirical approach)",
    "text": "2.9 Probability of an event (Empirical approach)\nEmpirical Probability is a type of probability that is calculated based on actual observations, experiments, or historical data rather than theoretical assumptions. It measures the likelihood of an event occurring by analyzing past occurrences or experimental results.\nFormula for Empirical Probability:\n\\[ P(E)=\\frac{Number\\ \\ of  \\ \\ times \\ \\ the\\ \\  event\\ \\  occurs}{Total \\ \\ number\\ \\  of \\ \\ trials} \\]\nWhere:\n\n\\(P(E)\\) is the probability of the event \\(E\\),\nThe numerator is the count of occurrences of the event, and\nThe denominator is the total number of trials or observations.\n\nExample 2.3: Suppose in a class there are 30 students; 20 are male and 10 are females. If a student is selected at random what is the probability that he is a male?\nSolution: Let, \\(A_1=\\) set of male students and \\(A_2=\\) set of female students. And, \\(\\Omega=\\) set of all students\nSo, probability that a male student is selected is:\n\\[\nP(A_1)=\\frac{n(A_1)}{n(\\Omega)}=\\frac{20}{30}=0.66667\\approx 0.67\n\\]\nInterpretation There is almost \\(67\\%\\) chance that the selected student will be male.\nExample 2.4: If 3 books are picked at random from a shelf containing 5 novels, 3 books of poems, and a dictionary, what is the probability that\n(a) the dictionary is selected?\n(b) 2 novels and 1 book of poems are selected?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html#properties-of-probability-laws",
    "href": "2Probability_2.html#properties-of-probability-laws",
    "title": "2¬† Probability",
    "section": "2.10 Properties of Probability Laws",
    "text": "2.10 Properties of Probability Laws\nProbability laws have a number of properties, which can be deduced from the axioms. Some of them are summarized below.\na) \\(P(A^C)=1-P(A)\\) [complement rule]\nb) \\(P(A \\cap B^C )=P(A)-P(A \\cap B)\\) [only A happens]\nc) \\(P(A \\cup B)=P(A)+P(B)-P(A \\cap B)\\) [additive rule]\nd) \\(P(A^C \\cap B^C )=P(A‚à™B)^C=1-P(A \\cup B)\\). [neither A NOR B happens]\nExample 2.5: In a class 65% students prefer tea and 35% students prefer coffee. While 15% students prefer both tea and coffee. If a student is selected at random from the class find the probability that\n\nhe/she prefers only coffee\nhe/she prefers tea or coffee\nhe/she prefers none (neither tea nor coffee)\n\nExample All Seasons Plumbing has two service trucks that frequently need repair. If the probability the first truck is available is 0.80, the probability the second truck is available is 0.60, and the probability that both trucks are available is .30, what is the probability neither truck is available?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html#conditional-probability",
    "href": "2Probability_2.html#conditional-probability",
    "title": "2¬† Probability",
    "section": "2.11 Conditional Probability",
    "text": "2.11 Conditional Probability\nThe conditional probability of an event \\(A\\), given an event \\(B\\) with \\(P(B) &gt; 0\\), is defined by,\n\\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n\\]Example 2.6 (Walpole et al. 2017a , 9th ed., page 63): Suppose that our sample space \\(\\Omega\\) is the population of adults in a small town who have completed the requirements for a college degree. We shall categorize them according to gender and employment status. The data are given in Table¬†2.1.\n\n\n\nTable¬†2.1: Categorization of the Adults in a Small Town\n\n\n\n\n\n\nEmployed\nUnployed\nTotal\n\n\n\n\nMale\n460\n40\n500\n\n\nFemale\n140\n260\n400\n\n\nTotal\n600\n300\n900\n\n\n\n\n\n\nA person is selected at random. What is the probability that the selected person is:\n(i) a Male\n(ii) a Female and employed\n(iii) a Male or unemployed\n(iv) a Male given that he is employed",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html#the-multiplication-rule",
    "href": "2Probability_2.html#the-multiplication-rule",
    "title": "2¬† Probability",
    "section": "2.12 The Multiplication Rule",
    "text": "2.12 The Multiplication Rule\nIf in an experiment the events A and B can both occur, then\n\\[\nP(A\\cap B)=P(A) P(B|A) ; \\ \\ provided \\ \\ P(A)&gt;0\n\\]\nIn general, assuming that all of the conditioning events (let, 3 events) have positive probability, we have\n\\[\nP(A_1\\cap A_2\\cap A_3)=P(A_1) P(A_2|A_1)P(A_3|A_1\\cap A_2)\n\\]\nExample 2.7 [Walpole et al. (2017a) , Example 2.36]: Suppose that we have a fuse box containing 20 fuses, of which 5 are defective. If 2 fuses are selected at random and removed from the box in succession without replacing the first, what is the probability that both fuses are defective?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html#probability-trees",
    "href": "2Probability_2.html#probability-trees",
    "title": "2¬† Probability",
    "section": "2.13 Probability trees",
    "text": "2.13 Probability trees\nConsider a sequential experiment where in the first stage either \\(A_1\\) or \\(A_2\\) can be happened with some probabilities . And in the second stage event \\(B\\) can be happened. If \\(B^C\\) is the complement of \\(B\\) then this experiment can be shown in the following tree diagram.\n\n\n\n\n\nExample 2.8: Two balls are drawn in succession, without replacement, from a box containing 3 blue and 2 white balls .\ni) What is the probability that both balls will be white?\nSolution: Here, two balls are drawn in succession (one by one) without replacement. This experiment can be shown in the following tree:\n\n\n\n\n\nThe probability of drawing a white ball on the first draw and a white ball on the second draw (both are white) is:\n\\(P(w_1\\cap w_2)=P(w_1) P(w_2|w_1)=(\\frac {2}{5}) (\\frac {1}{4})=\\frac {1}{10}\\)\nii) What is the probability that the second ball is white?\nSolution:\n\\(P(w_2)=P(w_1\\cap w_2)+P(b_1\\cap w_2)\\)\n\\(=P(w_1)P(w_2|w_1)+P(b_1)P(w_2|b_1)\\)\n\\(=(\\frac{2}{5})(\\frac{1}{4})+(\\frac{3}{5})(\\frac{2}{4})=\\frac{1}{10}+\\frac{3}{10}=\\frac{4}{10}=\\frac{2}{5}\\)\n*Example 2.9 [Walpole et al. (2017a), Example 2.37]: One bag contains 4 white balls and 3 black balls, and a second bag contains 3 white balls and 5 black balls. One ball is drawn from the first bag and placed unseen in the second bag. What is the probability that a ball now drawn from the second bag is black? (Hints: Apply probability tree)\nExample 2.10-Radar Detection (Bertsekas and Tsitsiklis 2008) : If an aircraft is present in a certain area, radar detects it and generates an alarm signal with probability 0.99. If an aircraft is not present the radar generates a (false) alarm, with probability 0.10. We assume that an aircraft is present with probability 0.05. What is the probability of no aircraft presence and a false alarm? What is the probability of aircraft presence and no detection?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html#independent-events",
    "href": "2Probability_2.html#independent-events",
    "title": "2¬† Probability",
    "section": "2.14 Independent events",
    "text": "2.14 Independent events\nIf two events A and B are independent, the probability that both of them occur is equal to the product of their individual probabilities i.e.\n\\[\nP(A\\cap B)=P(A) P(B)\n\\]\n\nCorollary: If A and B are independent events then their complement events also be independent that is,\n\n\\[\nP(A^C\\cap B^C)=P(A^C) P(B^C)\n\\]\n\nIndependence Rule for Multiple events:\n\n\\[\nP(A\\cap B \\cap C )=P(A) P(B) P(C)\n\\]\nExample 2.11 (Walpole et al. 2017b, Exercise 2.89) A town has two fire engines operating independently. The probability that a specific engine is available when needed is 0.96.\n(a) What is the probability that neither is available when needed?\n(b) What is the probability that a fire engine is available when needed?\nExample (Lind, Marchal, and Wathen 2012, 182) You take a trip by air that involves three independent flights. If there is an 80 percent chance each specific leg of the trip is done on time, what is the probability all three flights arrive on time?\nExample (Lind, Marchal, and Wathen 2012, 182) The probability a HP network server is down is .05. If you have three independent servers, what is the probability that at least one of them is operational?\nExample (Lind, Marchal, and Wathen 2012, 182) Twenty-two percent of all liquid crystal displays (LCDs) are manufactured by Samsung. What is the probability that in a collection of three independent LCD purchases, at least one is a Samsung?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html#system-reliability-montgomery2014b-page-38",
    "href": "2Probability_2.html#system-reliability-montgomery2014b-page-38",
    "title": "2¬† Probability",
    "section": "2.15 System Reliability (Montgomery and Runger 2014, 38)",
    "text": "2.15 System Reliability (Montgomery and Runger 2014, 38)\n\nSeries circuit: Suppose component \\(L\\) and \\(R\\) are connected in series from left to right . Also assume \\(L\\) and \\(R\\) operate ( or fail ) independently.\n\n\n\n\n\n\nThe probability that the circuit operates is\n\\[\nP(L \\ \\ and \\ \\ R)=P(L \\cap R)=P(L)P(R)=0.8*0.9=0.72\n\\]\nPractical interpretation: Notice that the probability that the circuit operates degrades to approximately 0.7 when all devices are required to be functional. The probability that each device is functional needs to be large for a circuit to operate when many devices are connected in series.\n\nParallel circuit: The following circuit operates only if there is a path of functional devices from left to right. The probability that each device functions is shown on the graph. Assume that devices fail independently. What is the probability that the circuit operates?\n\n\n\n\n\n\nLet \\(T\\) and \\(B\\) denote the events that the top and bottom devices operate, respectively. There is a path if at least one device operates. The probability that the circuit operates is\n\\[\nP(T \\ \\ or \\ \\ B) =P(T\\cup B)=1-P(T^C \\cap B^C)\n\\]\n\\[\n=1-P(T^C)P(B^C)=1-(0.05)(0.10)=1-0.005=0.995\n\\]\nPractical Interpretation: Notice that the probability that the circuit operates is larger than the probability that either device is functional. This is an advantage of a parallel architecture. A disadvantage is that multiple devices are needed.\nAdvance circuit The following circuit operates only if there is a path of functional devices from left to right. The probability that each device functions is shown on the graph. Assume that devices fail independently. What is the probability that the circuit operates? (Ans.: 0.9865)\n\n\n\n\n\n\nExample 2.12: The following circuit operates if and only if there is a path of functional devices from left to right. The probability that each device functions is as shown. Assume that the probability that a device is functional does not depend on whether or not other devices are functional. What is the probability that the circuit operates? (Ans.: 0.9293)\n\n\n\n\n\nExample 2.13: The following circuit operates if and only if there is a path of functional devices from left to right. The probability that each device functions is as shown. Assume that the probability that a device is functional does not depend on whether or not other devices are functional. What is the probability that the circuit operates? (Ans.: 0.9702)\n\n\n\n\n\n*Example 2.14: The following circuit operates if and only if there is a path of functional devices from left to right. Assume devices fail independently and that the probability of failure of each device is as shown. What is the probability that the circuit operates?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "2Probability_2.html#total-probability-theorem-and-bayes-rule",
    "href": "2Probability_2.html#total-probability-theorem-and-bayes-rule",
    "title": "2¬† Probability",
    "section": "2.16 Total Probability Theorem and Bayes‚Äô Rule",
    "text": "2.16 Total Probability Theorem and Bayes‚Äô Rule\nIn this section, we explore some applications of conditional probability. We start with the following theorem, which is often useful for computing the probabilities of various events, using a ‚Äúdivide-and-conquer‚Äù approach.\n\n2.16.1 Total Probability Theorem\nLet \\(A_1,...,A_n\\) be disjoint events that form a partition of the sample space (each possible outcome is included in one and only one of the events \\(A_1, . . . , A_n\\)) and assume that \\(P(A_i) &gt; 0\\), for all \\(i = 1,...,n\\). Then, for any event \\(B\\), we have\n\n\n\n\n\n\\[\nP(B)=P(A_1\\cap B)+P(A_2\\cap B)+....+P(A_n\\cap B)\n\\]\n\\[\n=P(A_1)P(B|A_1)+P(A_2)P(B|A_2)+....+P(A_n)P(B|A_n)\n\\]\nExample 2.15: Suppose that \\(A_1, A_2, A_3\\), and B are events where \\(A_1\\), \\(A_2\\), and \\(A_3\\) are mutually exclusive and \\(P(A_1) =0.2, P(A_2) =0.5, P(A_3) =0.3\\). Also given \\(P(B| A_1)=0.02, P(B|A_2)=0.05, P(B|A_3)=0.04\\). Find \\(P(B)\\).\n\n\n2.16.2 Bayes‚Äô Rule/Theorem\nLet \\(A_1,A_2,...,A_n\\) be disjoint events that form a partition of the sample space, and assume that \\(P(A_i) &gt; 0\\), for all \\(i\\). Then, for any event \\(B\\) such that \\(P(B) &gt; 0\\), we have\n\\[\nP(A_i|B)=\\frac{P(A_i \\cap B)}{P(B)}=\\frac{P(A_i)P(B|A_i)}{P(A_1)P(B|A_1)+P(A_2)P(B|A_2)+....+P(A_n)P(B|A_n)}\n\\]\nExample 2.16 (Walpole, 9th ed, Example 2.41,page 74): In a certain assembly plant, three machines, \\(B_1, B_2\\), and \\(B_3\\), make 30%, 45%, and 25%, respectively, of the products. It is known from past experience that 2%, 3%, and 2% of the products made by each machine, respectively, are defective. Now, suppose that a finished product is randomly selected. What is the probability that it is defective?\nExample 2.17 (Walpole, 9th ed, Example 2.42): With reference to Example 2.41, if a product was chosen randomly and found to be defective, what is the probability that it was made by machine B3?\nExample 2.18 (Walpole, 9th ed, Exercise 2.101) A paint-store chain produces and sells latex and semigloss paint. Based on long-range sales, the probability that a customer will purchase latex paint is 0.75. Of those that purchase latex paint, 60% also purchase rollers. But only 30% of semigloss paint buyers purchase rollers. A randomly selected buyer purchases a roller and a can of paint. What is the probability that the paint is latex?\nExample 2.19 (Radar Detection revisited): If an aircraft is present in a certain area, a radar detects it and generates an alarm signal with probability 0.99. If an aircraft is not present. the radar generates a (false) alarm, with probability 0.10. We assume that an aircraft is present with probability 0.05. What is the probability of no aircraft presence and a false alarm?\n\nWhat is the probability that the radar generates alarm?\nIf the radar generates alarm, what is the probability that there was an aircraft?\nIf the radar does not generate alarm, what is the probability that there was not any aircraft?\n\nExample 2.20(Montgomery, 6th ed., Exercise 2-179): An e-mail filter is planned to separate valid e-mails from spam. The word free occurs in 60% of the spam messages and only 4% of the valid messages. Also, 20% of the messages are spam. Determine the following probabilities:\n\nThe message contains free.\nThe message is spam given that it contains free.\nThe message is valid given that it does not contain free.\n\nExample 2.21: One urn has 3 blue and 2 white balls; a second urn has 1 blue and 3 white balls. A single fair die is rolled and if 1 or 2 comes up, a ball is drawn out of the first urn; otherwise, a ball is drawn out of the second urn. If the drawn ball is blue, what is the probability that it came out of the first urn? Out of the second urn?\n\n\n\n\n\n*Example 2.22: A binary communication channel carries data as one of two sets of signals denoted by 0 and 1. Owing to noise, a transmitted 0 is sometimes received as a 1, and a transmitted 1 is sometimes received as a 0. For a given channel, it can be assumed that a transmitted 0 is correctly received with probability 0.95 and a transmitted 1 is correctly received with probability 0.75. Also, 60% of all messages are transmitted as a 0. If a signal is sent, determine the probability that:\n\na 1 was received;\na 0 was received;\nan error occurred;\na 1 was transmitted given that a 1 was received ;\na 0 was transmitted given that a 0 was received.\n\n\n\n\n\nBertsekas, Dimitri P., and John N. Tsitsiklis. 2008. Introduction to probability. 2nd ed. Optimization and computation series. Belmont: Athena scientific.\n\n\nLind, Douglas A., William G. Marchal, and Samuel Adam Wathen. 2012. Statistical Techniques in Business & Economics. 15th ed. New York, NY: McGraw-Hill/Irwin.\n\n\nMontgomery, Douglas C., and George C. Runger. 2014. Applied Statistics and Probability for Engineers. Sixth edition. Hoboken, NJ: John Wiley; Sons, Inc.\n\n\nWalpole, Ronald E., Raymond H. Myers, Sharon L. Myers, and Keying Ye. 2017a. Probability & statistics for engineers & scientists: MyStatLab update. Ninth edition. Boston: Pearson.\n\n\n‚Äî‚Äî‚Äî, eds. 2017b. Probability & Statistics for Engineers & Scientists: MyStatLab Update. Ninth edition. Boston: Pearson.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "3Random_variable_3.html",
    "href": "3Random_variable_3.html",
    "title": "3¬† Random variable: Discrete",
    "section": "",
    "text": "3.1 Types of random variable\nThere are two important types of random variables, discrete and continuous.\nA discrete random variable is one whose possible values form a discrete set; in other words, the values can be ordered, and there are gaps between adjacent values. The random variable Y, just described, is discrete.\nIn contrast, the possible values of a continuous random variable always contain an interval, that is, all the points between some two numbers.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Random variable: Discrete</span>"
    ]
  },
  {
    "objectID": "3Random_variable_3.html#discrete-random-variable-and-probability-mass-function",
    "href": "3Random_variable_3.html#discrete-random-variable-and-probability-mass-function",
    "title": "3¬† Random variable: Discrete",
    "section": "3.2 Discrete random variable and Probability mass function",
    "text": "3.2 Discrete random variable and Probability mass function\nSuppose \\(X\\) is a discrete random variable. The probability mass function (PMF) of \\(X\\) can be denoted as \\(f(x)\\) where\n\\[\nf(x)=P(X=x)\n\\]For each possible outcome \\(x\\) ; \\(f(x)\\) must satisfies:\n\n\\[f(x) \\ge 0\\]\n\\[\\sum _x f(x)=1\\]\n\n\n3.2.1 The Cumulative Distribution Function of a Discrete Random Variable\nA function called the cumulative distribution function (CDF) specifies the probability that a random variable is less than or equal to a given value. The cumulative distribution function of the random variable \\(X\\) is the function\n\n\\(F(x) = P(X \\le x)\\)\n\nExample 3.1: Calculating probabilities A certain industrial process is brought down for recalibration whenever the quality of the items produced falls below specifications. Let \\(X\\) represent the number of times the process is recalibrated during a week, and assume that \\(X\\) has the following probability mass function.\n\n\n\n\\(x\\)\n0\n1\n2\n3\n4\n\n\n\\(f(x)\\)\n0.35\n0.25\n0.20\n0.15\n0.05\n\n\n\nCompute the following :\ni) \\(P(X=2)\\);\nii) \\(P(X&lt;3)\\) and \\(P(X&gt;2)\\);\niii) \\(F(2)\\)\nExample (Walpole et al. 2017, Example 3.8)A shipment of 20 similar laptop computers to a retail outlet contains 3 that are defective. If a school makes a random purchase of 2 of these computers, find the probability distribution for the number of defectives.\nSolution: Let \\(X\\) be a random variable whose values x are the possible numbers of defective computers purchased by the school. Then \\(x\\) can only take the numbers 0, 1, and 2. Now ,\n\\(P(X=0)=f(0)=P(N,N)=\\frac{\\binom{17}{2}\\binom{3}{0}}{\\binom{20}{2}}=\\frac{136}{190}\\)\n\\(P(X=1)=f(1)=P(D,N)=\\frac{\\binom{3}{1}\\binom{17}{1}}{\\binom{20}{2}}=\\frac{51}{190}\\)\n\\(P(X=2)=f(2)=P(D,D)=\\frac{\\binom{3}{2}\\binom{17}{0}}{\\binom{20}{2}}=\\frac{3}{190}\\)\nThus, the probability distribution of X is\n\n\n\n\\(x\\)\n0\n1\n2\n\n\n\\(f(x)\\)\n\\(\\frac{136}{190}\\)\n\\(\\frac{51}{190}\\)\n\\(\\frac{3}{190}\\)\n\n\n\nExample (Baron 2019, Exercise 3.1 (a)) A computer virus is trying to corrupt two files. The first file will be corrupted with probability 0.4. Independently of it, the second file will be corrupted with probability 0.3.\nCompute the probability mass function (PMF) of \\(X\\), the number of corrupted files.\nSolution: (Will be discussed in class)\n\n\n3.2.2 Expectation (Population Mean) of discrete random variable\nLet \\(X\\) be a discrete random variable with probability mass function \\(f(x) = P(X = x)\\).\nThe mean of \\(X\\) is given by\n\\[\\mu=\\sum_x x.f(x)\\]\nThe mean of \\(X\\) is sometimes called the expectation, or expected value, of X and may also be denoted by \\(E(X)\\) or by \\(\\mu\\).\nExample 3.2 : Let \\(X\\) represent the number of times the process is recalibrated during a week, and assume that \\(X\\) has the following probability mass function.\n\n\n\n\\(x\\)\n0\n1\n2\n3\n4\n\n\n\\(f(x)\\)\n0.35\n0.25\n0.20\n0.15\n0.05\n\n\n\nCompute the mean or expected value of \\(X\\).\nSolution:\nThe expected value of \\(X\\) is:\n\\[\n\\mu =E[X]=\\sum_{x=0}^4 x.f(x)\n\\]\n\\[\n=0(0.35)+1(0.25)+2(0.20)+3(0.15)+4(0.05)=1.30\n\\]\n\n\n3.2.3 Variance (population) of discrete random variable\nLet \\(X\\) be a discrete random variable with probability distribution \\(f(x)\\) and mean \\(\\mu\\). The variance of \\(X\\) is\n\\[\nvar(X)=\\sigma^2 =E[(X-\\mu)^2]=E(X^2)-\\mu^2\n\\] Where,\n\\[E(X^2)=\\sum_{x} x^2.f(x)\\]\n\nExample 3.2 : Let \\(X\\) represent the number of times the process is recalibrated during a week, and assume that \\(X\\) has the following probability mass function.\n\nCompute the expected value and variance of \\(X\\).\n\n\n\\(x\\)\n0\n1\n2\n3\n4\n\n\n\\(f(x)\\)\n0.35\n0.25\n0.20\n0.15\n0.05\n\n\n\nSolution: From Example 3.1, we have\nExpected value of \\(X\\) is \\(\\mu =E(X)=1.30\\)\nNow, \\[E(X^2)=\\sum_{x=0}^4 x^2.f(x)\\]\n\\(=0^2(0.35)+1^2 (0.25)+2^2 (0.20)+3^2 (0.15)+4^2 (0.05)\\)\n\\(=3.20\\)\nHence, \\(var(X)=\\sigma^2 =E(X^2)-\\mu^2=3.20-(1.30)^2=1.51\\)\n\nThe standard deviation is the square root of the variance:\n\\(\\sigma=\\sqrt {var(X)}\\)\n\n\n\n\n\n\n\nProperties of E(.) and var(.)\n\n\n\nIf \\(a\\) and \\(b\\) are constants, then\na) \\(E(b)=b\\)\nb) \\(E(aX+b)=aE(X)+b\\)\nc) \\(var(b)=0\\)\nd) \\(var(aX+b)=a^2 \\ \\ var (X)\\)\n\n\nExample Computer chips often contain surface imperfections. For a certain type of computer chip, the probability mass function of the number of defects X is presented in the following table.\n\n\n\n\\(x\\)\n0\n1\n2\n3\n4\n\n\n\n\n\\(f(x)\\)\n0.4\n0.3\n0.15\n0.1\n0.05\n\n\n\na. Find \\(P(X \\le 2)\\).\nb. Find \\(P(X &gt; 1)\\).\nc. Find expected value/average of \\(X\\).\nd. Find variance and standard deviation of \\(X\\).\ne. Also find \\(E(2X+5)\\) and \\(var(2X+5)\\).\n\n\n\n\n\nBaron, Michael. 2019. Probability and statistics for computer scientists. Third edition. A Chapman & Hall book. Boca Raton: London.\n\n\nWalpole, Ronald E., Raymond H. Myers, Sharon L. Myers, and Keying Ye. 2017. Probability & statistics for engineers & scientists: MyStatLab update. Ninth edition. Boston: Pearson.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Random variable: Discrete</span>"
    ]
  },
  {
    "objectID": "4Some_special_discrete_random_variables_4.html",
    "href": "4Some_special_discrete_random_variables_4.html",
    "title": "4¬† Some special discrete random variables",
    "section": "",
    "text": "4.1 Bernoulli r.v\nBernoulli r.v comes from Bernoulli trial-a trial which has TWO possible outcomes (success or failure).\nConsider the toss of a biased coin, which comes up a head with probability \\(p\\), and a tail with probability \\(1 - p\\). The Bernoulli random variable takes the two values 1 and 0, depending on whether the outcome is a head or a tail:\n\\[\nX=1 ; if \\ \\  a \\ \\ head,\\\\\nX=0 ;  if\\ \\  a \\ \\ tail.\n\\]\nThen the PMF of \\(X\\) is:\n\\[\nP(X=x)=f(x)=p^x(1-p)^{1-x}; \\ \\ x=0,1\n\\tag{4.1}\\]\nFor all its simplicity, the Bernoulli random variable is very important. In practice, it is used to model generic probabilistic situations with just two outcomes, such as:\n(a) The state of a telephone at a given time that can be either free or busy.\n(b) A person who can be either healthy or sick with a certain disease.\n(c) The preference of a person who can be either for or against a certain political candidate.\nFurthermore, by combining multiple Bernoulli random variables, one can construct more complicated random variables.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Some special discrete random variables</span>"
    ]
  },
  {
    "objectID": "4Some_special_discrete_random_variables_4.html#bernoulli-r.v",
    "href": "4Some_special_discrete_random_variables_4.html#bernoulli-r.v",
    "title": "4¬† Some special discrete random variables",
    "section": "",
    "text": "PMF:\n\n\n\nMean: \\(\\mu=E(X)=p\\)\nVariance: \\(\\sigma^2 =E(X-\\mu)^2=E(X^2)-\\mu^2=p(1-p)\\)\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of Mean and Variance of Bernoulli r.v\n\n\n\nMean:\n\\[E(X)=\\sum_{x=0}^1 x\\cdot f(x)=(0) f(0)+(1)f(1)=0+1\\cdotp p=p\\]\nVariance:\n\\(Var(X)=E(X^2)-[E(X)]^2=p-p^2=p(1-p)\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Some special discrete random variables</span>"
    ]
  },
  {
    "objectID": "4Some_special_discrete_random_variables_4.html#binomial-r.v",
    "href": "4Some_special_discrete_random_variables_4.html#binomial-r.v",
    "title": "4¬† Some special discrete random variables",
    "section": "4.2 Binomial r.v",
    "text": "4.2 Binomial r.v\nIn a Binomial experiment , the Bernoulli trial is repeated \\(n\\) times with the following conditions:\na) The trials are independent\nb) In each trial \\(P(success)=p\\) remains constant\nSuppose \\(X=number \\ \\ of \\ \\ successs \\ \\ in \\ \\ n\\ \\ trials\\). Then \\(X\\) is called a Binomial r.v or follows Binomial distribution.\nPMF: \\[P(X=x)=f(x)=\\binom {n}{x} p^x (1-p)^{n-x} ; x=0,1,2,...,n \\tag{4.2}\\]\nCDF: \\(P(X\\le x)=F(x)=f(0)+f(1)+...+f(x)\\)\nMean: \\(E(X)=np\\)\nVariance: \\(Var(X)=np(1-p)\\)\nWe write \\(X\\sim Bin (n,p)\\)\nIllustration\n\nConsider an experiment of tossing a biased coin 3(number of trials, n) times.\nTosses are independent, each toss has only TWO Outcomes-Head (Success) and Tail (Failure)\n\nThis type of trial is called the Bernoulli Trial\n\nSuppose, \\(P(H)=p\\) and remain constant in each toss, consequently, \\(P(T)=1-p=q\\) (let).\n\nSuppose, \\(X=\\# \\ \\ of\\ \\ head\\ \\ (successes)\\ \\  in\\ \\ 3\\ \\ tosses\\)\nNow, what is the probability that, we will have exactly 2 heads (success) in 3 tosses?\nThat is, \\(P(X=2)=?\\)\nNow, this can happen in the following ways:\n\\[P(X=2)=P(HHT)+P(HTH)+P(THH)\\] \\[=P(H)P(H)P(T)+P(H)P(T)P(H)+P(T)P(H)P(H)\\] [Since tosses are independent]\n\\[=p.p.q+p.q.p+q.p.p\\] \\[=p^2 q+p^2 q+p^2 q=3p^2 q\\] \\[\\therefore P(X=2)=\\binom{3}{2}p^2q^{3-2}\\] If, \\(p=0.6\\) is given, then we can easily compute \\(P(X=2)=f(2)\\). Now, if we repeat the toss 10 times \\((n=10)\\), with \\(P(H)=p\\), what is the value of \\(P(X=3)=f(3)\\)?\n\n\n\n\n\n\nNote\n\n\n\n\n\\(n\\) and \\(p\\) are said to be the parameters of the Binomial distribution.\n\\(f(x)=F(x)-F(x-1)\\) i.e \\(f(3)=F(3)-F(2)\\)\nIf \\(Y=number \\ \\ of \\ \\ failures\\ \\ in \\ \\ n\\ \\ trials\\) then \\(Y\\sim Bin(n,1-p)\\)\n\n\n\n\n\n\n\n\n\nRelation between Bernoulli r.v and Binomial r.v\n\n\n\nA Binomial Random Variable Is a Sum of Bernoulli Random Variables\nLet, \\(Y_i\\) is a Bernoulli r.v appeared in \\(i^{th}\\) Bernoulli trial. If we conduct \\(n\\) independent Bernoulli trials then we have \\(n\\) independent Bernoulli r.vs such as \\(Y_1, Y_2, ..., Y_n\\). Each \\(Y_i\\) has values of either \\(1\\) or \\(0\\).\nNow if \\(X\\) is a Binomial r.v then,\n\\[ X=Y_1+Y_2+...+Y_n =\\sum_{i=1}^n Y_i \\]\n\n\n\n\n\n\n\n\nDerivation of Mean and Variance of Binomial r.v\n\n\n\nFrom previous note, we know if \\(Y_i\\) is a Bernoulli r.v then\n\\(E(Y_i)=p\\) and \\(Var(Y_i)=p(1-p)\\)\nSo, the mean of Binomial r.v that is\n\\[ E(X)=E(Y_1+Y_2+...+Y_n) \\]\n\\[ =E(Y_1)+E(Y_2)+...+E(Y_n) \\]\n\\[ =p+p+...+p=np \\]\nNow, the variance of \\(X\\) is:\n\\[ Var(X)=Var(Y_1+Y_2+...+Y_n) \\]\n\\[ =Var(Y_1)+Var(Y_2)+...+Var(Y_n) \\]\n\\[ =p(1-p)+p(1-p)+...+p(1-p)=np(1-p) \\]\n\n\nProbability plot of binomial r.v for different values of \\(p\\) and shape characteristics\n\n\n\n\n\n\n\n\n\nFinding Binomial probability manually\nSuppose, \\(X\\sim Binom(n,p)\\); where \\(n=5\\) and \\(p=0.6\\). Find, (i) \\(P(X=2)\\) (ii) \\(P(X \\le 2)\\) (iii) \\(P(X\\ge3)\\).\nSolution:\nPMF of \\(X\\): \\(P(X=x)=f(x)=\\binom{5}{x} 0.6^x (0.4)^{5-x} \\ \\ ;x=0,1,2,...,5\\)\n(i) \\(P(X=2)=f(2)=\\binom{5}{2} 0.6^2 (0.4)^{5-2}=0.2304\\)\n(ii) \\(P(X \\le 2)=F(2)=f(0)+f(1)+f(2)=0.0102+0.0768+0.2304=0.3174\\)\n(iii) \\(P(X \\ge 3)=f(3)+f(4)+f(5)=0.6826\\)\nAlternative:(iii)\n\\(P(X \\ge 3)=1-P(X&lt; 3)=1-P(X \\le 2)=1-F(2)=1-0.3174=0.6826\\)\nFinding Binomial probability using Binomial Table\nIn the end of any Statistics book there are some Probability Distribution Table. We can use these table to compute the required probability for specific values of the parameters of certain probability distribution. Here I share the 1st page of Binomial distribution table (Baron 2019).\n\nSuppose, \\(X\\sim Binom(n,p)\\); where \\(n=5\\) and \\(p=0.6\\). Find, (i) \\(P(X=2)\\) (ii) \\(P(X \\le 2)\\) (iii) \\(P(X \\ge 3)\\) using Table.\nSolution:\n(i) \\(P(X=2)=f(2)=F(2)-F(1)=0.3174-0.0870=0.2304\\).\n(ii) \\(P(X\\le 2=F(2)=0.3174\\)\n(iii) \\(P(X \\ge 3)=1-P(X&lt; 3)=1-F(2)=1-0.3174=0.6826\\)\nExercise(Walpole et al. 2017)\n5.9 In testing a certain kind of truck tire over rugged terrain, it is found that 25% of the trucks fail to complete the test run without a blowout. Of the next 15 trucks tested, find the probability that: (a) from 3 to 6 have blowouts; (b) fewer than 4 have blowouts; (c) more than 5 have blowouts.\nSolution:\nLet, X= number of trucks that have blowouts\nGiven, \\(n=15; \\ \\ p=Pr(blowout)=0.25; \\ \\ q=1-p=0.75\\). Hence, \\(X\\sim Binom(n=15, p=0.25)\\), that is:\n\\[\nP(X=x)=f(x)=\\binom{15}{x}(0.25)^x (0.75)^{15-x}; x=0,1,2,...,15.\n\\]\nNow,\n(a) \\(P(3\\le X\\le 6)=f(3)+f(4)+f(5)+f(6)\\)=0.225+0.225+0.165+0.092=0.707.\nAlternative: \\(P(3\\le X\\le 6)=F(6)-F(2)=0.943-0.236=0.707\\) [from Table]\n(b) \\(P(X&lt;4)=f(0)+f(1)+f(2)+f(3)\\)=0.013+0.067+0.156+0.225=0.461.\nAlternative: \\(P(X&lt; 4)=F(3)=0.461\\) [from Table A2]\n(c) \\(P(X &gt; 5)=1-P(X \\le 5)=1-F(5)\\)=0.148\n5.12 A traffic control engineer reports that 75% of the vehicles passing through a checkpoint are from within the state. What is the probability that fewer than 4 of the next 9 vehicles are from out of state?\n5.16 Suppose that airplane engines operate independently and fail with probability equal to 0.4. Assuming that a plane makes a safe flight if at least one-half of its engines run, determine whether a 4-engine plane or a 2-engine plane has the higher probability for a successful flight.\n5.25 Suppose that for a very large shipment of integrated-circuit chips, the probability of failure for any one chip is 0.10. Assuming that the assumptions underlying the binomial distributions are met, find the probability that at most 3 chips fail in a random sample of 20.\nExercise(Montgomery and Runger 2014)\n3-93 Let \\(X\\) be a binomial random variable with \\(p = 0.1\\) . and \\(n = 10\\). Calculate the following probabilities from the binomial probability mass function and from the binomial table in Appendix A and compare results. (a) \\(P(X\\le 2)\\) (b) P(X&gt;8) (c) P(X = 4) (d) \\(P(5 \\le X \\le7)\\)\n3-115 The probability that a visitor to a Web site provides contact data for additional information is 0.01. Assume that 1000 visitors to the site behave independently. Determine the following probabilities: (a) No visitor provides contact data. (b) Exactly 10 visitors provide contact data. (c) More than 3 visitors provide contact data\nExercise(Baron 2019)\n3.21. A lab network consisting of 20 computers was attacked by a computer virus. This virus enters each computer with probability 0.4, independently of other computers. Find the probability that it entered at least 10 computers.\n3.22. Five percent of computer parts produced by a certain supplier are defective. What is the probability that a sample of 16 parts contains more than 3 defective ones?\nAnd so on‚Ä¶.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Some special discrete random variables</span>"
    ]
  },
  {
    "objectID": "4Some_special_discrete_random_variables_4.html#poisson-r.v",
    "href": "4Some_special_discrete_random_variables_4.html#poisson-r.v",
    "title": "4¬† Some special discrete random variables",
    "section": "4.3 Poisson r.v",
    "text": "4.3 Poisson r.v\nThe number of events occur randomly in an interval or in a region usually follows Poisson distribution. A famous French mathematician SimB4eon-Denis Poisson (1781‚Äì1840) first introduced this distribution.\nExample\nThe Poisson distribution may be useful to model variables like:\n\nThe no. of calls arrive at a customer care in 15 minites\nThe no. of arrivals at a car wash in one hour\nThe no. of repairs needed in 10 miles of highway\nThe no. of leaks in 100 miles of pipeline etc.\n\nUsually Poisson distribution is used to evaluate probability of ‚ÄúRare‚Äù event.\nThe probability mass function of the Poisson random variable \\(X\\), representing the number of outcomes occurring in a given time interval denoted by \\(t\\), is:\n\nPMF:\n\n\\[\nP(X=x)=f(x)=\\frac{e^{-\\lambda t}(\\lambda t)^x}{x!}; \\ \\ x=0,1,2,...,\\infty.\n\\tag{4.3}\\]\nHere, \\(\\lambda\\) is called arrival rate or average number of occurrences in long-run. And only parameter of Poisson distribution.\n\nMean: \\(\\mu=E(X)=\\lambda t\\)\nVariance: \\(\\sigma^2 =\\lambda t\\)\nWe write: \\(X\\sim Pois(\\lambda t)\\)\n\nN.B: The mean and variance of Poisson random random variable are identical. This is the unique property of Poisson r.v.\nProbability of plot of poisson r.v for different values of \\(\\lambda\\) (for a fixed interval \\(t=1\\))\n\n\n\n\n\n\n\n\n\nWe can see that, for small \\(\\lambda\\) the distribution of Poisson r.v is positively skewed and as the value of \\(\\lambda\\) increases the distribution tends to symmetry.\nFinding Poisson probability\nConsider a discrete r.v say \\(X\\sim Pois(\\lambda t)\\). Suppose, \\(\\lambda =1.5\\) and \\(t=2\\). Find, (i) P(X=4) (ii)\\(P(X \\le 2)\\) (iii) \\(P(X\\ge3)\\).\nSolution:\nPMF of \\(X\\): \\(P(X=x)=f(x)=\\frac{e^{-\\lambda t}(\\lambda t)^x}{x!}; x=0,1,...,\\infty.\\)\n(i) For \\(t=2\\) , \\(\\mu=\\lambda t=1.5*2=3\\).\nSo, \\(P(X=4)=f(4)=\\frac{e^{-3}(3)^4}{4!}=0.1680\\)\n(ii) \\(P(X\\le2)=\\sum_{x=0}^{2}f(x)=\\sum_{x=0}^{2}\\frac{e^{-3}(3)^x}{x!}=e^{-3}[\\frac{3^0}{0!}+\\frac{3^1}{1!}+\\frac{3^2}{2!}]=0.4232\\)\n(iii) \\(P(X \\ge 3)=1-P(X&lt; 3)=1-P(X\\le 2)=1-0.423=0.5768\\)\nFinding Poisson probability using Table\nWe can use Poisson distribution table to compute Poisson probabilities. Here I share the 1st page of Poisson distribution table (Baron 2019).\n\nConsider a discrete r.v say \\(X\\sim Pois(\\lambda t)\\). Suppose, \\(\\lambda =1.5\\) and \\(t=2\\). Find, (i) \\(P(X \\le 2)\\) (ii)P(X=4)\nSolution by using Table:\nFor \\(t=2\\) , \\(\\mu=\\lambda t=1.5*2=3\\).\n(i) \\(P(X \\le 2)=F(2)=0.423\\)\n[For x=2 and \\(\\mu \\ \\ or \\ \\lambda =3\\); corresponding probability in Table A3 is 0.423]\n(ii) \\(P(X=4)=f(4)=F(4)-F(3)=0.815-0.647=0.168\\)\nExample 5.17:(Walpole et al. 2017) During a laboratory experiment, the average number of radioactive particles passing through a counter in 1 millisecond is 4. What is the probability that 6 particles enter the counter in a given millisecond?\nExample 5.18:(Walpole et al. 2017) Ten is the average number of oil tankers arriving each day at a certain port. The facilities at the port can handle at most 15 tankers per day. What is the probability that on a given day tankers have to be turned away?\nExample 3.8:[(Pishro-Nik 2014)] The number of emails that I get in a weekday can be modeled by a Poisson distribution with an average of 0.2 emails per minute.\n\nWhat is the probability that I get no emails in an interval of length 5 minutes?\nWhat is the probability that I get more than 3 emails in an interval of length 10 minutes?\n\nSolution\nLet, \\(X\\)=number of emails that I get in a given interval.\nGiven, \\(\\lambda =0.2 \\ \\ min^{-1}\\).\n\\(X\\) will follow \\(Pois(\\lambda t)\\)\n1. In this case \\(\\mu=\\lambda t=0.2*5=1\\). So, \\(P(X=0)=f(0)=e^{-\\mu}=e^{-1}=0.3679\\).\n2. In this case \\(\\mu=\\lambda t=0.2*10=2\\). So,\\(P(X&gt;3)=1-P(X\\le 3)=1-F(3)=1-0.857=0.143\\). [From Table A3]\nApproximation of Binomial Distribution to Poisson\nWhen,\n\n\\(p \\rightarrow0\\) (Success rate is very low);\n\\(n\\rightarrow \\infty\\) (Number of trials is very large);\n\nThen Binomial distribution can be approximated by Poisson distribution.\n\nMathematically, \\(Binom (x; n,p)\\approx Pois(\\lambda)\\); where \\(\\lambda=np\\).\n\nN.B: In practical situation if \\(n \\ge 30\\) and \\(p\\le 0.05\\) ;hence \\(q\\ge 0.95\\),then the approximation is close enough to use the Poisson distribution for binomial problems(Baron 2019).\nExample 5.20:(Walpole et al. 2017) In a manufacturing process where glass products are made, defects or bubbles occur, occasionally rendering the piece undesirable for marketing. It is known that, on average, 1 in every 1000 of these items produced has one or more bubbles. What is the probability that a random sample of 8000 will yield fewer than 7 items possessing bubbles?\nSolution:\nLet,\\(X=number\\ \\ of \\ \\ glasses\\ \\ possesing\\ \\ bubbles\\)\nGiven, \\(Pr(buuble \\ \\ occurs)=p=1/1000=0.001\\) which is less than \\(0.05\\), and \\(n=8000\\) which is greater than \\(30\\). So, the PMF of \\(X\\) can be approximated by Poisson distribution with\n\\[\\lambda =np=8000*0.001=8\\] that is \\(X\\sim Pois (\\lambda=8)\\)\nAccording to question,\n\\(P(X&lt;7)=f(0)+f(1)+...+f(6)=F(6)=0.313\\) (Ans.)\n[By using Table A3]\nExercise 5.87:(Walpole et al. 2017) Imperfections in computer circuit boards and computer chips lend themselves to statistical treatment. For a particular type of board, the probability of a diode failure is 0.03 and the board contains 200 diodes.\n\nWhat is the mean number of failures among the diodes? (Ans: \\(\\mu=np=200*0.03=6\\))\nWhat is the variance?(Ans: \\(\\sigma^2=np(1-p)=200*0.03*(1-0.03)=5.82\\))\nThe board will work if there are no defective diodes. What is the probability that a board will work? Ans (c): The board will work if there are no defective diodes. So, P(The board will work)=\\(P(X=0)=f(0)=e^{-\\mu}=e^{-6}=0.0025\\).\n\n\n\n\n\nBaron, Michael. 2019. Probability and statistics for computer scientists. Third edition. A Chapman & Hall book. Boca Raton: London.\n\n\nMontgomery, Douglas C., and George C. Runger. 2014. Applied Statistics and Probability for Engineers. Sixth edition. Hoboken, NJ: John Wiley; Sons, Inc.\n\n\nPishro-Nik, Hossein. 2014. Introduction to Probability, Statistics, and Random Processes. Wroclaw: Amazon Fulfillment/Kappa Research.\n\n\nWalpole, Ronald E., Raymond H. Myers, Sharon L. Myers, and Keying Ye. 2017. Probability & statistics for engineers & scientists: MyStatLab update. Ninth edition. Boston: Pearson.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Some special discrete random variables</span>"
    ]
  },
  {
    "objectID": "5Continuous_rv_5.html",
    "href": "5Continuous_rv_5.html",
    "title": "5¬† Continuous r.v and probability density function",
    "section": "",
    "text": "5.1 Definition\nA continuous r.v \\(X\\) must have a probability density function (PDF) \\(f(x)\\) such that\n1) \\(f(x) \\ge 0\\) [Non-negativity]\n2) \\(\\int_{x\\in \\mathbb{R}} f(x)dx =1\\) [Total AREA under the curve \\(f(x)\\) always 1]\n3) \\(P(a&lt;X&lt;b)=\\int_a^b f(x) dx\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Continuous r.v and probability density function</span>"
    ]
  },
  {
    "objectID": "5Continuous_rv_5.html#illustration-with-an-example",
    "href": "5Continuous_rv_5.html#illustration-with-an-example",
    "title": "5¬† Continuous r.v and probability density function",
    "section": "5.2 Illustration with an example",
    "text": "5.2 Illustration with an example\nGiven \\(f(x)=\\frac{1}{2}x \\ \\ ; 0\\le x\\le 2\\)\na) Show/plot the graph of \\(f(x)\\).\nb) Is \\(f(x)\\) a PDF?\nc) Find \\(P(X&lt;1.0)\\).\nd) Find \\(P(X=1.0)\\)\nSolution:\n(a)\n\n\n\n\n\n\n\n\n\nb) Here, \\(f(x)\\ge 0\\) for all values of \\(x\\) in the interval \\(0\\le x\\le2\\).\nNow, total area under curve \\(f(x)\\) from \\(x=0\\) to \\(x=2\\) is\n\\(\\int_{0}^2 f(x)dx\\)\n\\(=AREA \\ \\ of\\ \\  the\\ \\  SHADED\\ \\  Triangle\\)\n\n\n\n\n\n\n\n\n\n\\[\n=\\frac{1}{2} \\times base\\times height\n\\]\n\\[\n=\\frac{1}{2} \\times 2\\times 1=1\n\\]\nSo, total area under curve \\(f(x)\\) is \\(1\\) that is \\(\\int_{0}^{2} f(x)dx=1\\).\nHence, \\(f(x)\\) is a PDF.\nc) Here,\n\\[\nP(X&lt;1)=Area \\ \\ under\\ \\ the \\ \\ curve \\ \\ from \\ \\ x=0 \\ \\ to  \\ \\ x=1\n\\]\n\\[\n=Area \\ \\ of \\ \\ the \\ \\ SHADED \\ \\ Triangle\n\\]\n\n\n\n\n\n\n\n\n\n\\[\n=\\frac{1}{2}\\times 1 \\times f(1)=\\frac{1}{2}\\times 1 \\times 0.5=0.25\n\\]\nTherefore \\(P(X&lt;1)=0.25\\)\nd) \\(P(X=1.0)=0\\) [Because there is no area at \\(x=1.0\\)]\n\n\n\n\n\n\nNote\n\n\n\nWe always remember that Probability in an interval of \\(X\\) is actually the \\(AREA\\) under the pdf \\(f(x)\\).\n\n\nProblem 6.2.1 A random variable has the following density function.\n\\[\nf(x)=1-0.5x \\ \\ ; \\ \\ 0&lt;x&lt;2\n\\]\na) Graph the density function.\nb) Verify that \\(f(x)\\) is a density function.\nc) Fond \\(P(X&gt;1)\\).\nd) Find \\(P(X&lt;0.5)\\).\ne) Find \\(P(X=1.5)\\).\nN.B: \\(P(X=a)=0\\) as well as \\(P(X=b)=0\\). So, \\(P(X\\le a )\\) is same as \\(P(X&lt;a)\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Continuous r.v and probability density function</span>"
    ]
  },
  {
    "objectID": "5Continuous_rv_5.html#cdf-of-continuous-r.v-x",
    "href": "5Continuous_rv_5.html#cdf-of-continuous-r.v-x",
    "title": "5¬† Continuous r.v and probability density function",
    "section": "5.3 CDF of continuous r.v \\(X\\)",
    "text": "5.3 CDF of continuous r.v \\(X\\)\nBy definition, CDF,\n\\[\nF(x)=P(X\\le x)= \\int_{-\\infty}^{x} f(x)dx\n\\]\nTherefore,\n\n\\(f(x)=\\frac{d}{dx} F(x)\\).\n\\(P(a&lt;X&lt;b)=F(b)-F(a)\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Continuous r.v and probability density function</span>"
    ]
  },
  {
    "objectID": "5Continuous_rv_5.html#expectation-and-variance-of-continuous-r.v",
    "href": "5Continuous_rv_5.html#expectation-and-variance-of-continuous-r.v",
    "title": "5¬† Continuous r.v and probability density function",
    "section": "5.4 Expectation and variance of continuous r.v",
    "text": "5.4 Expectation and variance of continuous r.v\nIf \\(X\\) is a continuous r.v with PDF \\(f(x)\\) then\nExpected value of \\(X\\) is\n\\[\n\\mu=E(X)= \\int_{x\\in \\mathbb{R}} x\\cdot f(x)dx\n\\] Variance of \\(X\\) is\n\\[\nVar(X)=E(X^2)-\\mu^2=\\int_{x\\in \\mathbb{R}} x^2\\cdot f(x)dx-\\mu^2\n\\]\nExample 3.11(Walpole et al. 2017) Suppose that the error in the reaction temperature, in \\(^0C\\), for a controlled laboratory experiment is a continuous random variable X having the probability density function\n\\[\nf(x)=\\frac{x^2}{3}; -1&lt;x&lt;2.\n\\]\n\nVerify that \\(f(x)\\) is a density function.\nFind \\(P(0&lt; X \\le 1)\\).\n\nExample 3.12(Walpole et al. 2017) Find \\(F(x)\\), and use it to evaluate \\(P(0 &lt; X\\le1)\\).\nH.W: Find \\(E(X)\\) and \\(Var(X)\\) where,\\(f(x)=\\frac{x^2}{3}; -1&lt;x&lt;2\\).\nExercise 3.29(Walpole et al. 2017) An important factor in solid missile fuel is the particle size distribution. Significant problems occur if the particle sizes are too large. From production data in the past, it has been determined that the particle size (in micrometers) distribution is characterized by\n\\[\nf(x)=3x^{-4}; x&gt; 1\n\\]\n\nVerify that this is a valid density function.\nEvaluate \\(F(x)\\).\nWhat is the probability that a random particle from the manufactured fuel exceeds 4 micrometers?\n\nExercise 3.69(Walpole et al. 2017) The life span in hours of an electrical component is a random variable with cumulative distribution function\n\\[\nF(x)=1-e^{-\\frac{x}{50}}; x&gt;0\n\\]\n\nDetermine its probability density function (PDF).\nDetermine the probability that the life span of such a component will exceed 70 hours.\n\nExercise 3.36 (Walpole et al. 2017) On a laboratory assignment, if the equipment is working, the density function of the observed outcome, \\(X\\), is\n\\[\nf(x)=2(1-x) \\ \\ ; 0&lt;x&lt;1\n\\]\n\nCalculate \\(P(X\\le 1)\\)\nWhat is the probability that \\(X\\) will exceed 0.5?\nGiven that \\(X \\ge 0.5\\), what is the probability that \\(X\\) will be less than 0.75?\n\nHints: We can solve this exercise by either using \\(F(x)\\) or simply drawing function \\(f(x)\\).\n\n\n\n\nWalpole, Ronald E., Raymond H. Myers, Sharon L. Myers, and Keying Ye. 2017. Probability & statistics for engineers & scientists: MyStatLab update. Ninth edition. Boston: Pearson.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Continuous r.v and probability density function</span>"
    ]
  },
  {
    "objectID": "6Some_special_continuous_rv.html",
    "href": "6Some_special_continuous_rv.html",
    "title": "6¬† Some special continuous random variables",
    "section": "",
    "text": "6.1 Uniform distribution/r.v\nA continuous r.v \\(X\\) is said to be uniform r.v ranges between \\(a\\) to \\(b\\) if it has the following PDF\n\\[\nf(x)=\\frac{1}{b-a} \\ \\ ; \\ \\ a&lt;x&lt;b\n\\tag{6.1}\\]\nwith\nMean: \\(\\mu=E(X)=\\frac{a+b}{2}\\)\nVariance: \\(\\sigma^2=\\frac{(b-a)^2}{12}\\)\nWe write, \\(X\\sim U(a,b)\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Some special continuous random variables</span>"
    ]
  },
  {
    "objectID": "6Some_special_continuous_rv.html#uniform-distributionr.v",
    "href": "6Some_special_continuous_rv.html#uniform-distributionr.v",
    "title": "6¬† Some special continuous random variables",
    "section": "",
    "text": "Figure¬†6.1: Graph of f(x)\n\n\n\n\n\n\n\n\n6.1.1 Finding probability for uniform r.v\nIf \\(X\\sim U(a,b)\\) then the \\(P(x_1&lt;X&lt;x_2)\\) is actually the area of the shaded rectangle.\n\n\n\nComputing area for an interval of Uniform distribution\n\n\nThat is,\n\\[\nP(x_1&lt;X&lt;x_2)=Base\\times Height=(x_2-x_1)\\times \\frac{1}{b-a}\n\\]\nProblem 1 The phase angle, \\(\\Theta\\), of the signal at the input to a modem is uniformly distributed between \\(0\\) and \\(2\\pi\\) radians.\na) What are the PDF, expected value, and variance of \\(\\Theta\\)?\nb) Find the probability that phase angle exceeds \\(\\frac{3\\pi}{2}\\)?\nProblem 2 In a radio communications system, the phase difference \\(X\\) between the transmitter and receiver is modeled as having a uniform density in \\([‚Äî\\pi, +\\pi]\\). Find \\(P(X &lt; 0)\\) and \\(P(X&lt; \\pi/2)\\).\n\nProblem 3(Navidi 2011, 278) Resistors are labeled 100 \\(\\Omega\\). In fact, the actual resistances are uniformly distributed on the interval \\((95, 103)\\).\na. Find the mean resistance.\nb. Find the standard deviation of the resistances.\nc. Find the probability that the resistance is between \\(98\\) and \\(102 \\ \\ \\Omega\\).\nd. Suppose that resistances of different resistors are independent. What is the probability that three out of six resistors have resistances greater than \\(100\\) \\(\\Omega\\)?\n\n\n6.1.2 Generating Uniform random number in R\n\npar(mar=c(4,4,1,1)) # Adjust graph margin\nset.seed(911) \nu=runif(1000,10,30) # Generating 1000 random numbers from U(10,30)\nhist(u,main = \" \",col=\"steelblue\",ylim = c(0,120)) # Frequency histogram of U\n\n\n\n\n\n\n\nFigure¬†6.2: Histogram of Uniform random numbers from U(10,30), \\(\\ n=1000\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Some special continuous random variables</span>"
    ]
  },
  {
    "objectID": "6Some_special_continuous_rv.html#normal-or-gaussian-r.v",
    "href": "6Some_special_continuous_rv.html#normal-or-gaussian-r.v",
    "title": "6¬† Some special continuous random variables",
    "section": "6.2 Normal or Gaussian r.v",
    "text": "6.2 Normal or Gaussian r.v\nThe most important probability distribution for describing a continuous random variable is the normal probability distribution. The normal distribution has been used in a wide variety of practical applications in which the random variables are heights and weights of people, test scores, scientific measurements, amounts of rainfall, and other similar values.\n\n6.2.1 Definition\nA continuous r.v \\(X\\) is said to be normal r.v if it has the following PDF:\n\\[\nf(x)=\\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} (\\frac{x-\\mu}{\\sigma})^2}\\ \\ ; -\\infty&lt;x&lt;\\infty\n\\tag{6.2}\\]\nThe graph of \\(f(x)\\) is called normal curve (Figure¬†6.3).\n\n\n\n\n\n\n\n\nFigure¬†6.3: Normal Curve\n\n\n\n\n\nMean: \\(E(X)=\\mu\\)\nVariance: \\(Var(X)=\\sigma^2\\)\nWe write: \\(X\\sim N(\\mu , \\sigma^2)\\)\nProperties of normal distribution\n\nThe total area under the normal curve \\(f(x)\\) is 1 that is\n\\[\n\\int_{-\\infty}^{\\infty} f(x)dx=1\n\\]\nNormal distribution is symmetric about mean, \\(\\mu\\)\nMean, median and mode is identical in normal distribution that is \\(Mean=Median=Mode=\\mu\\)\nAlmost \\(99\\%\\) observations of \\(X\\) lie within 3 standard deviation of mean that is\n\\[\nP(\\mu-3\\sigma&lt;X&lt;\\mu+3\\sigma)\\approx 0.99\n\\]\nAlmost \\(95\\%\\) observations of \\(X\\) lie within 2 standard deviation of mean that is \\[\nP(\\mu-2\\sigma&lt;X&lt;\\mu+2\\sigma)\\approx 0.95\n\\]\nAlmost \\(68\\%\\) observations of \\(X\\) lie within 1 standard deviation of mean that is \\[\nP(\\mu-\\sigma&lt;X&lt;\\mu+\\sigma)\\approx 0.68\n\\]\n\n\n\n6.2.2 Standard normal r.v\nSuppose \\(X\\sim N(\\mu, \\sigma^2)\\). Then the variable \\(Z=\\frac{X-\\mu}{\\sigma}\\) is said to be standard normal variable with PDF\n\\[\nf(z)=\\frac{1}{\\sqrt {2\\pi}} e^{-z^2} \\ \\ ; -\\infty&lt;z&lt;\\infty\n\\tag{6.3}\\]\nMean: \\(E(Z)=0\\)\nVariance: \\(Var(Z)=1\\)\nWe write: \\(Z\\sim N(0,1)\\)\n\n\n\n\n\n\n\n\nFigure¬†6.4: Standard normal curve\n\n\n\n\n\n\n\n6.2.3 Computing probability(area) under standard normal curve\nTo compute area (probability) under the standard normal curve for a given interval of \\(z\\) we use standard Normal Distribution table which provides cumulative probabilities.\nRULE-I: Suppose we want to find \\(P(Z&lt;1.25)\\).\nFrom TABLE A.2 in Navidi (2011) we have\n\\[\nP(Z&lt;1.25)=0.8944\n\\]\n\nThe probability \\(P(Z&lt;1.25)\\) is shown in Figure¬†6.5.\n\n\n\n\n\n\n\n\nFigure¬†6.5: Area under standard normal curve for Z&lt;1.25\n\n\n\n\n\nRULE-II: Now we find \\(P(Z&gt;1.36)\\)\nSo, due to symmetry we can write \\(P(Z&gt;1.36)=P(Z&lt;-1.36)=0.0869\\)\n\n\n\n\n\n\n\n\n\nFigure¬†6.6: Area under standard normal curve for Z&gt;1.36\n\n\n\n\n\nRULE-III: Let us evaluate \\(P(-1.96&lt;Z&lt;2.58)\\).\nWe can write\n\\[\n=P(-1.96&lt;Z&lt;2.58)\n\\]\n\\[\n=P(Z&lt;2.58)-P(Z&lt;-1.96)\n\\]\n\\[\n=0.9951-0.0250=0.9701\n\\]\n\n\n\n\n\n\n\n\nFigure¬†6.7: Area under standard normal curve for -1.96&lt;Z&lt;2.58\n\n\n\n\n\n\n\n6.2.4 Finding quantiles (percentiles, quartiles, deciles etc.) of \\(Z\\)\nWhat is the \\(90^{th}\\) percentile of \\(Z\\)? To answer this question, let \\(k\\) is the \\(90^{th}\\) percentile of \\(Z\\). So we can write\n\\[\nP(Z&lt;k)=0.90 \\ \\  \\ \\ \\ \\ \\ \\ \\ \\cdot \\cdot \\cdot (1)\n\\]\nFrom TABLE A.2 in Navidi (2011) we have\n\\[\nP(Z&lt;1.28)=0.90 \\ \\  \\ \\ \\ \\ \\ \\ \\ \\cdot \\cdot \\cdot(2)\n\\]\nComparing eq.(1) with eq.(2) we have \\(k=1.28\\). So the \\(90^{th}\\) percentile of \\(Z\\) is \\(1.28\\).\nProblem 1 Find \\(c\\) such that \\(P(Z&gt;c)=0.05\\).\nProblem 2 Find \\(c\\) such that \\(P(-c&lt;Z&lt;c)=0.95\\).\n\n\n6.2.5 Computing probability(area) under normal curve:\nSuppose \\(X\\sim N(30, 5^2)\\) . Then find the following:\na) \\(P(X&lt;22)\\)\nb) \\(P(X&gt;44)\\)\nc) \\(P(20&lt;X&lt;35)\\)\nd) If \\(P(X&lt;x)=0.25\\) then find the value of \\(x\\).\nSolution:\nHere, \\(\\mu=30\\) and \\(\\sigma=5\\)\n\na) \\(P(X&lt;22)=P(\\frac{X-\\mu}{\\sigma}&lt;\\frac{22-30}{5})=P(Z&lt;-1.60)=0.0548\\).\n\nb) \\(P(X&gt;44)=P(\\frac{X-\\mu}{\\sigma}&gt;\\frac{44-30}{5})\\)\n\\(=P(Z&gt;2.80)=P(Z&lt;-2.80)=0.0026\\)\n\nc) \\(P(20&lt;X&lt;35)=P(\\frac{20-30}{5}&lt;\\frac{X-\\mu}{\\sigma}&lt;\\frac{35-30}{5})\\)\n\\(=P(-2&lt;Z&lt;1)=P(Z&lt;1)-P(Z&lt;-2)\\)\n\\(=0.8413-0.0228=0.8185\\)\n\nd) To find the value of \\(x\\) we proceed this way.\n\\[\nP(X&lt;x)=0.25\n\\] \\[\n\\implies P(\\frac{X-\\mu}{\\sigma}&lt;\\frac{x-30}{5})=0.25\n\\]\n\\[\n\\implies P(Z&lt;\\frac{x-30}{5})=0.25 \\ \\ \\ \\ \\ \\cdot \\cdot \\cdot (1)\n\\]\nFrom TABLE (Appendix B) we have\n\\[\nP(Z&lt;-0.67)=0.25  \\ \\ \\ \\ \\cdot \\cdot \\cdot (2)\n\\]\nComparing (1) with (2) we can write\n\\[\n\\frac{x-30}{5}=-0.67\n\\] \\[\n\\implies x=30+(-0.67)\\times 5\n\\]\n\\[\n\\therefore x=26.65\n\\]\n\n\n\n\n\n\nNote\n\n\n\nIf \\(P(X&lt;x)=p\\) and\n\\(P(Z&lt;z)=p\\) then\n\\[\nx=\\mu+z\\sigma\n\\]\n\n\n\n\n6.2.6 Applications of the Normal Distribution (Walpole et al. 2017)\nExample 6.7: A certain type of storage battery lasts, on average, 3.0 years with a standard deviation of 0.5 year. Assuming that battery life is normally distributed, find the probability that a given battery will last less than 2.3 years.\nExample 6.8 : An electrical firm manufactures light bulbs that have a life, before burn-out, that is normally distributed with mean equal to 800 hours and a standard deviation of 40 hours. Find the probability that a bulb burns between 778 and 834 hours.\nExample 6.9 : In an industrial process, the diameter of a ball bearing is an important measurement. The buyer sets specifications for the diameter to be \\(3.0 \\pm 0.01\\) cm. The implication is that no part falling outside these specifications will be accepted. It is known that in the process the diameter of a ball bearing has a normal distribution with mean \\(\\mu = 3.0\\) and standard deviation \\(\\sigma = 0.005\\). On average, how many manufactured ball bearings will be scrapped?\nExample 6.10 : Gauges are used to reject all components for which a certain dimension is not within the specification \\(1.50 \\pm d\\). It is known that this measurement is normally distributed with a mean of 1.50 and a standard deviation of 0.2. Determine the value d such that the specifications ‚Äúcover‚Äù 95% of the measurements.\nSolution:\nLet, \\(X=measurement \\ \\ of \\ \\ certain \\ \\ dimension\\)\nGiven, \\(X\\sim N(1.5,0.2)\\)\nAccording to question,\n\\(P(1.5-d&lt;X&lt;1.5+d)=0.95\\)\nSo, \\(P(X&lt;1.5-d)+P(X&gt;1.5+d)=0.05\\)\n\nSo, \\(P(X &lt;1.5-d)=0.025  \\ \\ \\ \\ \\cdot \\cdot \\cdot(1)\\)\n\\(\\implies P(\\frac{X-\\mu}{\\sigma} &lt;\\frac{1.5-d-1.5}{0.2})=0.025\\)\n\\(\\implies P(Z&lt;\\frac{-d}{0.2})=0.025 \\ \\ \\ \\ \\cdot \\cdot \\cdot(1)\\)\nFrom TABLE A.2 we have\n\\(P(Z&lt;-1.96)=0.025 \\ \\ \\ \\ \\ \\cdot \\cdot\\cdot(2)\\)\nComparing eq.(1) with eq. (2) we have\n\\(\\frac{-d}{0.2}=-1.96\\)\n\\(\\implies -d=-0.392\\)\n\\(\\therefore d=0.392\\)\nAlternative:\n\\(P(X &lt;1.5-d)=0.025  \\ \\ \\ \\ \\cdot \\cdot \\cdot(1)\\)\nBut, \\(P(Z&lt;-1.96)=0.025 \\ \\ \\ \\ \\cdot \\cdot \\cdot (2)\\)\nHence, \\(1.5-d=\\mu+z\\sigma\\); where \\(z=-1.96\\)\n\\(\\implies 1.5-d=1.5-1.96\\times 0.2\\)\n\\(\\therefore d=0.392\\)\nExercise 6.11 : A soft-drink machine is regulated so that it discharges an average of 200 milliliters per cup. If the amount of drink is normally distributed with a standard deviation equal to 15 milliliters,\n\nwhat fraction of the cups will contain more than 224 milliliters?\nwhat is the probability that a cup contains between 191 and 209 milliliters?\nhow many cups will probably overflow if 230-milliliter cups are used for the next 1000 drinks?\nbelow what value do we get the smallest 25% of the drinks?\n\nExercise 6.14 The finished inside diameter of a piston ring is normally distributed with a mean of 10 centimeters and a standard deviation of 0.03 centimeter.\n\nWhat proportion of rings will have inside diameters exceeding 10.075 centimeters?\nWhat is the probability that a piston ring will have an inside diameter between 9.97 and 10.03 centimeters?\nBelow what value of inside diameter will 15% of the piston rings fall?\n\nExercise 6.17 : The average life of a certain type of small motor is 10 years with a standard deviation of 2 years. The manufacturer replaces free all motors that fail while under guarantee. If she is willing to replace only 3% of the motors that fail, how long a guarantee should be offered? Assume that the lifetime of a motor follows a normal distribution.\n\n\n6.2.7 Normal approximation to Binomial distribution\nBinom tends to normal when \\(n \\rightarrow \\infty\\) and \\(p\\rightarrow 0.5\\)\n\n\n\n\n\n\n\n\nFigure¬†6.8: Histogram of \\(Bin(x; n=6,p=0.2)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†6.9: Histogram of \\(Bin(x; n=15,p=0.2)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†6.10: Histogram of \\(Bin(x; n=50,p=0.2)\\)\n\n\n\n\n\nFrom Figure¬†6.8, Figure¬†6.9 and Figure¬†6.10 we observe that as \\(n\\) getting large with \\(p=0.2\\) (which is not extremely close to 0) the binomial distribution can be approximated to normal distribution.\nSo, when \\(n\\) become large and \\(p\\) is not extremely small (close to 0) or extremely large (close to 1) the normal approximation is most useful in calculating binomial sums.\n\n\n\n\n\n\nNormal Approximation to the Binomial Distribution\n\n\n\n\nLet X be a binomial random variable with parameters \\(n\\) and \\(p\\). For large \\(n\\), \\(X\\) has approximately a normal distribution with \\(\\mu = np\\) and \\(\\sigma^2 =np(1-p)\\).\nThe approximation will be good if \\(np\\) and \\(n(1-p)\\) are greater than or equal to \\(5\\) (Walpole et al. 2017).\n\n\n\nContinuity correction\nThis correction is needed when we approximate a discrete distribution (Binomial in this case) by a continuous distribution (Normal). Recall that the probability \\(P(X = x)\\) may be positive if \\(X\\) is discrete, whereas it is always \\(0\\) for continuous \\(X\\). This is resolved by introducing a continuity correction. Expand the interval by 0.5 units in each direction, then use the Normal approximation (Baron 2019).\n\n\n\nTable¬†6.1: Continuity correction for binomial probabilities\n\n\n\n\n\nBinomial Probability\nWith continuity correction\n\n\n\n\n\\(P(X=x)\\)\n\\(P(x-0.5&lt;X&lt;x+0.5)\\)\n\n\n\\(P(X\\le x)\\)\n\\(P(X\\le x+0.5)\\)\n\n\n\n\n\n\nExample 6.15 (Walpole et al. 2017, 191): The probability that a patient recovers from a rare blood disease is 0.4. If 100 people are known to have contracted this disease, what is the probability that fewer than 30 survive?",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Some special continuous random variables</span>"
    ]
  },
  {
    "objectID": "6Some_special_continuous_rv.html#exponential-r.v",
    "href": "6Some_special_continuous_rv.html#exponential-r.v",
    "title": "6¬† Some special continuous random variables",
    "section": "6.3 Exponential r.v",
    "text": "6.3 Exponential r.v\nIn many situations, such as when modeling waiting times, inter-arrival times, the lifespan of hardware, breakdown times, and the intervals between phone calls, the exponential distribution is utilized. The time (suppose \\(T\\)) between rare events in Poisson process with arrival rate \\(\\lambda\\) (number of arrival per unit time) can be treated as exponential r.v.\nThe exponential r.v \\(T\\) has the following PDF\n\\[\nf(t)=\\lambda e^{-\\lambda t} \\ \\ ; \\ \\ t&gt;0\n\\tag{6.4}\\]\n\nCDF: \\(F(t)=P(T\\le t)=P(T&lt; t)=1-e^{-\\lambda t}; t&gt; 0\\)\nHence, \\(P(T&gt; t)=1-P(T\\le t)=1-F(t)=e^{-\\lambda t}\\)\nMean: \\(E(T)=\\frac{1}{\\lambda}\\)\nVariance: \\(Var(T)=\\frac{1}{\\lambda^2}\\)\nWe write, \\(T\\sim Exp(\\lambda)\\)\n\n\n\n\n\n\n\n\n\nFigure¬†6.11: Exponential probability density functions for selected values of \\(\\lambda\\)\n\n\n\n\n\nThe quantity \\(\\lambda\\) is a parameter of Exponential distribution, and its meaning is clear from \\(E(T) = \\frac{1}{\\lambda}\\) . If T is time, measured in minutes, then \\(\\lambda\\) is a frequency, measured in \\(min^{-1}\\). For example, if arrivals occur every half a minute, on the average, then \\(E(T) = 0.5\\)min and \\(\\lambda=2\\), saying that they occur with a frequency (arrival rate) of 2 arrivals per minute. This \\(\\lambda\\) has the same meaning as the parameter of Poisson distribution(Baron 2019).\nExample 4.5(Baron 2019) Jobs are sent to a printer at an average rate of 3 jobs per hour.\n\nWhat is the expected time between jobs?\nWhat is the probability that the next job is sent within 5 minutes?\n\nSolution: Given, number of jobs per hour, \\(\\lambda=3\\ \\ hr^{-1}\\) per hour. Let, \\(T\\)=time elapsed between jobs (hour).\nSo, \\(T\\sim Exp(\\lambda)\\)\n\n\\(E(T)=\\frac{1}{\\lambda} hr=\\frac{1}{3} hr=20\\ \\ mins\\);\nHere, \\(5 \\ \\ mins=\\frac{5}{60} hr=\\frac{1}{12} hr\\) We know, \\(F(t)=1-e^{-\\lambda t}; t&gt;0\\)\n\nSo, \\(P(T&lt;5 \\ \\ mins)=P(T&lt;\\frac{1}{12})=F(\\frac{1}{12})=1-e^{-3*\\frac{1}{12}}=0.22\\)\nExample 4.58 (Navidi 2011) A radioactive mass emits particles according to a Poisson process at a mean rate of 15 particles per minute. At some point, a clock is started. What is the probability that more than 5 seconds will elapse before the next emission? What is the mean waiting time until the next particle is emitted?\nSolution\nLet, \\(T=elapsed \\ \\ time \\ \\ before \\ \\ the \\ \\ next \\ \\ emission (in \\ \\ second)\\)\nGiven, \\(\\lambda=15 min^{-1}=\\frac{15}{60} s^{-1}=0.25 s^{-1}\\) and\n\\(T\\sim Exp(\\lambda)\\);\n\\(P(T\\le t)=F(t)=1-e^{-\\lambda t}\\)\nP(more than 5 seconds will elapse before the next emission)=\\(P(T&gt;5)=e^{-\\lambda * 5}=e^{-0.25*5}=0.2865\\)\nMean waiting time, \\(E(T)=\\frac{1}{\\lambda} s=\\frac{1}{0.25}s=4s\\)\n\n6.3.1 Lack of Memory Property\nIf \\(T \\sim Exp(\\lambda)\\), and \\(t\\) and \\(s\\) are positive numbers, then\n\\[\nP(T&gt; t+s| T&gt; s)=P(T&gt; t)\n\\]\nWe can also express the Lack of memory property in this way:\n\\[\nP(T&lt; t+s| T&gt; s)=P(T&lt; t)\n\\]\nThe probability that we must wait additional \\(t\\) units, given that we have already waited \\(s\\) units, is the same as the probability that we must wait \\(t\\) units from the start. The exponential distribution does not ‚Äúremember‚Äù how long we have been waiting.\n\nIn particular, if the lifetime of a component follows the exponential distribution, then the probability that a component that is \\(s\\) time units old will last an additional \\(t\\) time units is the same as the probability that a new component will last \\(t\\) time units.\nIn other words, a component whose lifetime follows an exponential distribution does not show any effects of age or wear(Navidi 2011).\nBut if the failure of the component is a result of gradual or slow wear (as in mechanical wear), then the exponential does not apply and either the gamma or the Weibull distribution (see (Walpole et al. 2017), Section 6.10) may be more appropriate.\n\nExample 4.59(Navidi 2011) The lifetime of a particular integrated circuit has an exponential distribution with mean 2 years. Find the probability that the circuit lasts longer than three years.\nExample 4.60(Navidi 2011) Refer to Example 4.59. Assume the circuit is now four years old and is still functioning. Find the probability that it functions for more than three additional years (Hints: Apply Lack of Memory Property).\nExercises for Section 4.7(Navidi 2011)\n1.Let \\(T \\sim Exp(0.45)\\). Find \\(\\mu_T, \\sigma^2_T, P(T&gt;3)\\) and the median of \\(T\\).\n2.The time between requests to a web server is exponentially distributed with mean 0.5 seconds.\n\nWhat is the value of the parameter \\(\\lambda\\)?\nWhat is the median time between requests?\nWhat is the standard deviation?\nWhat is the 80th percentile?\nFind the probability that more than one second elapses between requests.\nIf there have been no requests for the past two seconds, what is the probability that there more than one additional second will elapse before the next request?\n\nSolution\nLet, \\(T=time \\ \\ between \\ \\ requests \\ \\ in \\ \\ second\\)\nIf, \\(T\\sim Exp(\\lambda)\\); then it is given that\n\\(E(T)=0.5 \\ \\ second\\)\n\\(\\implies \\frac{1}{\\lambda}=0.5 \\ \\ second\\)\na. \\(\\therefore \\lambda=2 s^{-1}\\)\nb. If \\(M\\) is the median time between request then,\n\\(P(T \\le M)=0.5\\)\n\\(\\implies F(M)=0.5\\)\n\\(\\implies 1-e^{-\\lambda *M}=0.5\\)\n\\(\\implies e^{-\\lambda *M}=0.5\\)\n\\(\\implies {-\\lambda *M}=ln(0.5)\\)\n\\(\\implies M =\\frac{ln(0.5)}{-\\lambda}=\\frac{ln(0.5)}{-2}=0.3466\\approx0.35\\)\nSo, median time between request is \\(0.35s\\)\nc. Standard deviation of \\(T\\), \\(\\sigma_T=\\frac{1}{\\lambda}s=1/2 =0.5 s\\)\nd. Let, \\(P_{80}\\) denotes 80th percentile.\nSo, solve the following equation for \\(P_{80}\\)\n\\(P(T\\le P_{80})=0.80\\)\n\\(\\implies P(T&gt;P_{80})=0.20\\)\n\\(\\implies e^{-\\lambda \\times P_{80}}=0.20\\)\n\\(\\implies P_{80}=\\frac {ln(0.20)}{-\\lambda}=0.805\\)\n\\(\\therefore P_{80}=0.805 s\\)\ne. \\(P(T&gt;1)=e^{-\\lambda *1}=0.1353\\)\nf. If there have been no requests for the past two seconds, the probability that there more than one additional second will elapse before the next request is:\n\\(P(T&gt;1+2 /T&gt;2)=P(T&gt;1)=e^{-\\lambda *1}=0.1353\\) [by using Lack of memory property]\n8.A radioactive mass emits particles according to a Poisson process at a mean rate of 2 per second. Let T be the waiting time, in seconds, between emissions.\n\nWhat is the mean waiting time?\nWhat is the median waiting time?\nFind \\(P(T &gt; 2)\\). Hint:\\(P(T &gt; 2)=e^{-\\lambda *2}\\)\nFind \\(P(T &lt; 0.1)\\).Hint:\\(P(T &lt; 0.1)=F(0.1)\\)\nFind \\(P(0.3&lt; T &lt; 1.5)\\). Hint:\\(P(0.3&lt; T &lt; 1.5)=F(1.5)-F(0.3)\\)\nIf 3 seconds have elapsed with no emission, what is the probability that there will be an emission within the next second? (Use Lack of Memory Property)\n\nSolution of f.\nSince T is exponentially distributed and hold lack of memory property, so it dose not matter what was happened in past 3 seconds. We have to just compute that there will be an emission (event will occur) within the next second. So,\n\\(P(T&lt;1)=F(1)=1-e^{-\\lambda*1}\\) (do yourself)\n\n\n6.3.2 Generating exponential random numbers in R\n\npar(mar=c(4,4,1,1)) # Adjust graph margin\nset.seed(911) \nt=rexp(1000,rate = 2.5)\nhist(t, main = \"\",col = \"steelblue\")\n\n\n\n\n\n\n\nFigure¬†6.12: Histogram of exponential random variable from Exp(\\(\\lambda=2.5)\\),\\(\\ n=1000\\)\n\n\n\n\n\n\n\n\n\nBaron, Michael. 2019. Probability and statistics for computer scientists. Third edition. A Chapman & Hall book. Boca Raton: London.\n\n\nNavidi, William Cyrus. 2011. Statistics for Engineers and Scientists. 3rd ed. New York: McGraw-Hill.\n\n\nWalpole, Ronald E., Raymond H. Myers, Sharon L. Myers, and Keying Ye. 2017. Probability & statistics for engineers & scientists: MyStatLab update. Ninth edition. Boston: Pearson.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Some special continuous random variables</span>"
    ]
  },
  {
    "objectID": "7Further_Topics_on_R.vs.html",
    "href": "7Further_Topics_on_R.vs.html",
    "title": "7¬† Further topics on random variables",
    "section": "",
    "text": "7.1 Transformation of random variables",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Further topics on random variables</span>"
    ]
  },
  {
    "objectID": "7Further_Topics_on_R.vs.html#joint-probability-distributions",
    "href": "7Further_Topics_on_R.vs.html#joint-probability-distributions",
    "title": "7¬† Further topics on random variables",
    "section": "7.2 Joint Probability Distributions",
    "text": "7.2 Joint Probability Distributions\nWhen events are happened simultaneously, to explore the relationship between two random variables we need joint probability distributions.\nDefinition\n\n\n\n\n\n\nThe function \\(f(x,y)\\) is said to be a joint density function of the continuous random variables \\(X\\) and \\(Y\\) if\n\n\\(f(x,y)\\ge0,\\) for all \\((x,y)\\)\n\\(\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty f(x,y)\\ \\ dx\\ \\ dy=1\\)\n\\(P[(X,Y)\\in A]=\\int \\int _A f(x,y) \\ \\ dx \\ \\ dy,\\) for any region \\(A\\) in the plane \\(xy\\).\n\n\n\n\n\nRPython\n\n\n\nset.seed(42)\n\nhist(rnorm(1000),freq = FALSE,ylim = c(0,.4),breaks = 10,col = \"steelblue\")\nlines(density(rnorm(1000)),col=\"blue\",lwd=2)\n\n\n\n\n\n\n\nFigure¬†7.1\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n#import pandas as pd\n\n# Generate 1000 samples\nseed = 42\n\nn_rv = np.random.normal(loc=0, scale=1, size=1000)\n#print(n_rv)\n\n\n#plt.clf()  # Clears the current figure\n\n## Using `seaborn`\n\nsns.histplot(n_rv, kde=True,stat=\"probability\",bins=10)\nplt.title(\"Histogram of Normal Distribution\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure¬†7.2: Histogram of Normal Distribution",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Further topics on random variables</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "8¬† Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Baron, Michael. 2019. Probability and statistics for computer\nscientists. Third edition. A Chapman & Hall book. Boca Raton:\nLondon.\n\n\nBertsekas, Dimitri P., and John N. Tsitsiklis. 2008. Introduction to\nprobability. 2nd ed. Optimization and computation series. Belmont:\nAthena scientific.\n\n\nLind, Douglas A., William G. Marchal, and Samuel Adam Wathen. 2012.\nStatistical Techniques in Business & Economics. 15th ed.\nNew York, NY: McGraw-Hill/Irwin.\n\n\nMontgomery, Douglas C., and George C. Runger. 2014a. Applied\nStatistics and Probability for Engineers. Sixth edition. Hoboken,\nNJ: John Wiley; Sons, Inc.\n\n\n‚Äî‚Äî‚Äî. 2014b. Applied Statistics and Probability for Engineers.\nSixth edition. Hoboken, NJ: John Wiley; Sons, Inc.\n\n\n‚Äî‚Äî‚Äî. 2014c. Applied Statistics and Probability for Engineers.\nSixth edition. Hoboken, NJ: John Wiley; Sons, Inc.\n\n\nNavidi, William Cyrus. 2011. Statistics for Engineers and\nScientists. 3rd ed. New York: McGraw-Hill.\n\n\nPishro-Nik, Hossein. 2014. Introduction to Probability, Statistics,\nand Random Processes. Wroclaw: Amazon Fulfillment/Kappa Research.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nWalpole, Ronald E., Raymond H. Myers, Sharon L. Myers, and Keying Ye.\n2017b. Probability & statistics for engineers & scientists:\nMyStatLab update. Ninth edition. Boston: Pearson.\n\n\n‚Äî‚Äî‚Äî, eds. 2017a. Probability & Statistics for Engineers &\nScientists: MyStatLab Update. Ninth edition. Boston: Pearson.",
    "crumbs": [
      "References"
    ]
  }
]